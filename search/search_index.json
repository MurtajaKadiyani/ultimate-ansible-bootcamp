{
    "docs": [
        {
            "location": "/", 
            "text": "Ultimate Ansible Bootcamp\n\n\n\n\n Chapter1: Introduction to Ansible\n\n\nChapter2: Path to Automation\n\n\n Chapter3 : Setting up Learning Environment \n\n\nChapter 4: Ad Hoc Server Management  Management\n\n\n Chapter 5: Playbooks - Learning to Write Infrastructure as a Code \n\n\nChapter 6: Working with Roles \n\n\nChapter 7: Variables and Templates\n\n\nChapter 8: Ansible Galaxy\n\n\nChapter 9: Control Structures\n\n\nChapter10: Magic Variables and Multiple Environments\n\n\nChapter11: Vault\n\n\nChapter12: Deployment\n\n\n\n\nLicense (CC-BY-NC-ND)\n\n\nUltimate Ansible Bootcamp\n by \nSchool of Devops\n is licensed under a \nCreative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#ultimate-ansible-bootcamp", 
            "text": "Chapter1: Introduction to Ansible  Chapter2: Path to Automation   Chapter3 : Setting up Learning Environment   Chapter 4: Ad Hoc Server Management  Management   Chapter 5: Playbooks - Learning to Write Infrastructure as a Code   Chapter 6: Working with Roles   Chapter 7: Variables and Templates  Chapter 8: Ansible Galaxy  Chapter 9: Control Structures  Chapter10: Magic Variables and Multiple Environments  Chapter11: Vault  Chapter12: Deployment", 
            "title": "Ultimate Ansible Bootcamp"
        }, 
        {
            "location": "/#license-cc-by-nc-nd", 
            "text": "Ultimate Ansible Bootcamp  by  School of Devops  is licensed under a  Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License .", 
            "title": "License (CC-BY-NC-ND)"
        }, 
        {
            "location": "/setup/", 
            "text": "Settig up Learning Environment\n\n\nOption 1: Setting up a Vagrant based environment\n\n\nInstall VirtualBox and Vagrant\n\n\n\n\n\n\n\n\nTOOL\n\n\nVERSION\n\n\nLINK\n\n\n\n\n\n\n\n\n\n\nVirtualBox\n\n\n5.1.26\n\n\nhttps://www.virtualbox.org/wiki/Downloads\n\n\n\n\n\n\nVagrant\n\n\n1.9.7\n\n\nhttps://www.vagrantup.com/downloads.html\n\n\n\n\n\n\n\n\nImporting a  VM Template\n\n\nvagrant box list\n\nvagrant box add ansible  codespace-ansible-ubuntu1604-9.box\n\nvagrant box list\n\n\n\n\n\nProvisioning Vagrant Nodes\n\n\nClone repo if not already\n\n\ngit clone https://github.com/schoolofdevops/lab-setup.git\n\n\n\n\n\n\nLaunch environments with Vagrant\n\n\ncd lab-setup/ansible/codespace\n\nvagrant up\n\n\n\n\n\nLogin to node\n\n\nTerminal\n\n\ncd lab-setup/ansible/codespace\nvagrant ssh\nsudo su\n\n\n\n\n\nYou could also visit  \n to access the codespaces env.\n\n\nOption 2: Setting up a codespaces environment  with Docker\n\n\n\n\nClone the git repo\n\n\n\n\ngit clone https://github.com/codespaces-io/codespaces.git\n\n\n\n\nStart Codespaces IDE\n\n\nAfter installing Docker-Engine and Docker-Compose, change directory into the corresponding tool you want to learn. For example, let us assume that you want to learn puppet. In that case,\n\n\ncd cs-ansible\n\n\n\n\nThen all you need to do is to run\n\n\ndocker-compose up -d\n\n\n\n\nThis single command will initialize your Codespaces IDE.\n\n\nUse Codespaces IDE\n\n\nTo use Codespaces IDE,\n\n\n\n\nOpen your browser.\n\n\nVisit your machine's IP with port 8000. (Ex. http://192.168.0.60:8080)\n\n\nYou will be asked for your e-mail address. Enter it and you are good to go.\n\n\n\n\n\n\n\n\nNow you will be presented with the Codespaces IDE console.", 
            "title": "Setting up the Environment"
        }, 
        {
            "location": "/setup/#settig-up-learning-environment", 
            "text": "", 
            "title": "Settig up Learning Environment"
        }, 
        {
            "location": "/setup/#option-1-setting-up-a-vagrant-based-environment", 
            "text": "", 
            "title": "Option 1: Setting up a Vagrant based environment"
        }, 
        {
            "location": "/setup/#install-virtualbox-and-vagrant", 
            "text": "TOOL  VERSION  LINK      VirtualBox  5.1.26  https://www.virtualbox.org/wiki/Downloads    Vagrant  1.9.7  https://www.vagrantup.com/downloads.html", 
            "title": "Install VirtualBox and Vagrant"
        }, 
        {
            "location": "/setup/#importing-a-vm-template", 
            "text": "vagrant box list\n\nvagrant box add ansible  codespace-ansible-ubuntu1604-9.box\n\nvagrant box list", 
            "title": "Importing a  VM Template"
        }, 
        {
            "location": "/setup/#provisioning-vagrant-nodes", 
            "text": "Clone repo if not already  git clone https://github.com/schoolofdevops/lab-setup.git  Launch environments with Vagrant  cd lab-setup/ansible/codespace\n\nvagrant up  Login to node  Terminal  cd lab-setup/ansible/codespace\nvagrant ssh\nsudo su  You could also visit    to access the codespaces env.", 
            "title": "Provisioning Vagrant Nodes"
        }, 
        {
            "location": "/setup/#option-2-setting-up-a-codespaces-environment-with-docker", 
            "text": "Clone the git repo   git clone https://github.com/codespaces-io/codespaces.git", 
            "title": "Option 2: Setting up a codespaces environment  with Docker"
        }, 
        {
            "location": "/setup/#start-codespaces-ide", 
            "text": "After installing Docker-Engine and Docker-Compose, change directory into the corresponding tool you want to learn. For example, let us assume that you want to learn puppet. In that case,  cd cs-ansible  Then all you need to do is to run  docker-compose up -d  This single command will initialize your Codespaces IDE.", 
            "title": "Start Codespaces IDE"
        }, 
        {
            "location": "/setup/#use-codespaces-ide", 
            "text": "To use Codespaces IDE,   Open your browser.  Visit your machine's IP with port 8000. (Ex. http://192.168.0.60:8080)  You will be asked for your e-mail address. Enter it and you are good to go.     Now you will be presented with the Codespaces IDE console.", 
            "title": "Use Codespaces IDE"
        }, 
        {
            "location": "/ad-hoc/", 
            "text": "Getting Started with Ansible (Ad Hoc Server Management)\n\n\nCreating Project Specific Ansible Configuration\n\n\nThe default configurations for ansible resides at /etc/ansible/ansible.cfg. Instead of relying on defaults, we are going to creates  a custom configuration file for our project. The advantage with that is we could take this configurations on any host and execute it the same way, without touching the default system configurations.  This custom configurations will essentially  override the values in /etc/ansible/ansible/cfg.\n\n\nAnsible configuration file\n\n\nChange into /vagrant/code/chap3 directory on your ansible host. Create a file called ansible.cfg  Add  the following contents to the file.\n\n\nOn Ansible Control node,\n\n\ncd chap4\n\n\n\n\nCreate \nansible.cfg\n in chap4\n\n\n[defaults]\nremote_user = devops\ninventory   = environments/prod\nretry_files_save_path = /tmp\nhost_key_checking = False\nlog_path=~/ansible.log\n\n\n\n\n\nCreating Host Inventory\n\n\nSince you are going to create a environment specific inventory,\ncreate a \nenvironments\n directory and a  file  inside it called \nprod\n\n\nmkdir environments\n\n\n\n\n\nCreate inventory file\n\n\nfile: environments/prod\n\n\nLet's create three groups as follows,\n\n\n[local]\nlocalhost ansible_connection=local\n\n[lb]\nlb\n\n[app]\napp1\napp2\n\n\n[db]\ndb\n\n[prod:children]\nlb\napp\ndb\n\n\n\n\n\n\n\nFirst group contains the localhost, the control host. Since it does not need to be connected over ssh, it mandates we add ansible_connection=local option\n\n\nSecond group contains  Application Servers. We will add  two app servers to this group.\n\n\nThird group holds the information about the database servers.\n\n\n\n\nThe inventory file should look like below.\n\n\nAnsible ping\n\n\nWe will use Ansible to make sure all the hosts are reachable\n\n\nansible all -m ping\n\n\n\n\n\n[Output]\n\n\nlb | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\napp1 | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\napp2 | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\ndb | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\n\n\n\n\nAd Hoc commands\n\n\nTry running following \nfire-and-forget\n Ad-Hoc commands...\n\n\nRun \nhostname\n command on all hosts\n\n\nLet us print the hostname of all the hosts\n\n\nansible all -a hostname\n\n\n\n\n[output]\n\n\nlocalhost | SUCCESS | rc=0 \n\nansible\n\n192.168.61.11 | SUCCESS | rc=0 \n\ndb\n\n192.168.61.12 | SUCCESS | rc=0 \n\napp\n\n192.168.61.13 | SUCCESS | rc=0 \n\napp\n\n\n\n\nCheck the \nuptime\n\n\nHow long the hosts are \nup\n?\n\n\nansible all -a uptime\n\n\n\n\n[Output]\n\n\nlocalhost | SUCCESS | rc=0 \n\n 13:17:13 up  2:21,  1 user,  load average: 0.16, 0.03, 0.01\n\n192.168.61.12 | SUCCESS | rc=0 \n\n 13:17:14 up  1:50,  2 users,  load average: 0.00, 0.00, 0.00\n\n192.168.61.13 | SUCCESS | rc=0 \n\n 13:17:14 up  1:47,  2 users,  load average: 0.00, 0.00, 0.00\n\n192.168.61.11 | SUCCESS | rc=0 \n\n 13:17:14 up  1:36,  2 users,  load average: 0.00, 0.00, 0.00\n\n\n\n\nCheck memory info on app servers\n\n\nDoes my app servers have any disk space \nfree\n?\n\n\nansible app -a free\n\n\n\n\n[Output]\n\n\n192.168.61.13 | SUCCESS | rc=0 \n\n             total       used       free     shared    buffers     cached\nMem:        372916     121480     251436        776      11160      46304\n-/+ buffers/cache:      64016     308900\nSwap:      4128764          0    4128764\n\n192.168.61.12 | SUCCESS | rc=0 \n\n             total       used       free     shared    buffers     cached\nMem:        372916     121984     250932        776      11228      46336\n-/+ buffers/cache:      64420     308496\nSwap:      4128764          0    4128764\n\n\n\n\nInstalling packages\n\n\nLet us \ninstall\n Docker on app servers\n\n\nansible app -a \nyum install -y docker-engine\n\n\n\n\n\nThis command will fail.\n\n\n[Output]\n\n\n192.168.61.13 | FAILED | rc=1 \n\nLoaded plugins: fastestmirror, prioritiesYou need to be root to perform this command.\n\n192.168.61.12 | FAILED | rc=1 \n\nLoaded plugins: fastestmirror, prioritiesYou need to be root to perform this command.\n\n\n\n\nRun the fillowing command with sudo permissions.\n\n\nansible app -s -a \nyum install -y docker-engine\n\n\n\n\n\nThis will install docker in our app servers\n\n\n[Output]\n\n\n192.168.61.12 | SUCCESS | rc=0 \n\nLoaded plugins: fastestmirror, priorities\nSetting up Install Process\nLoading mirror speeds from cached hostfile\n * base: mirrors.nhanhoa.com\n * epel: mirror.rise.ph\n * extras: mirror.fibergrid.in\n * updates: mirror.fibergrid.in\n283 packages excluded due to repository priority protections\nResolving Dependencies\n--\n Running transaction check\n---\n Package docker-engine.x86_64 0:1.7.1-1.el6 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version              Repository          Size\n================================================================================\nInstalling:\n docker-engine       x86_64       1.7.1-1.el6          local_docker       4.5 M\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 4.5 M\nInstalled size: 19 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : docker-engine-1.7.1-1.el6.x86_64                             1/1\n  Verifying  : docker-engine-1.7.1-1.el6.x86_64                             1/1\n\nInstalled:\n  docker-engine.x86_64 0:1.7.1-1.el6\n\nComplete!\n\n192.168.61.13 | SUCCESS | rc=0 \n\nLoaded plugins: fastestmirror, priorities\nSetting up Install Process\nLoading mirror speeds from cached hostfile\n * base: mirror.fibergrid.in\n * epel: mirror.rise.ph\n * extras: mirror.fibergrid.in\n * updates: mirror.fibergrid.in\n283 packages excluded due to repository priority protections\nResolving Dependencies\n--\n Running transaction check\n---\n Package docker-engine.x86_64 0:1.7.1-1.el6 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version              Repository          Size\n================================================================================\nInstalling:\n docker-engine       x86_64       1.7.1-1.el6          local_docker       4.5 M\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 4.5 M\nInstalled size: 19 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : docker-engine-1.7.1-1.el6.x86_64                             1/1\n  Verifying  : docker-engine-1.7.1-1.el6.x86_64                             1/1\n\nInstalled:\n  docker-engine.x86_64 0:1.7.1-1.el6\n\nComplete!\n\n\n\n\nRunning commands one machine at a time\n\n\nDo you want a command to run on \none machine at a time\n ?\n\n\nansible all -f 1 -a \nfree\n\n\n\n\n\nUsing \nmodules\n to manage the state of infrastructure\n\n\nCreating users and groups using \nuser\n and \ngroup\n\n\nTo create a group\n\n\nansible app -s -m group -a \nname=admin state=present\n\n\n\n\n\nThe output will be,\n\n\n192.168.61.13 | SUCCESS =\n {\n    \nchanged\n: true,\n    \ngid\n: 501,\n    \nname\n: \nadmin\n,\n    \nstate\n: \npresent\n,\n    \nsystem\n: false\n}\n192.168.61.12 | SUCCESS =\n {\n    \nchanged\n: true,\n    \ngid\n: 501,\n    \nname\n: \nadmin\n,\n    \nstate\n: \npresent\n,\n    \nsystem\n: false\n}\n\n\n\n\nTo create a user\n\n\nansible app -s -m user -a \nname=devops group=admin createhome=yes\n\n\n\n\n\nThis will create user \ndevops\n,\n\n\n192.168.61.13 | SUCCESS =\n {\n    \nchanged\n: true,\n    \ncomment\n: \n,\n    \ncreatehome\n: true,\n    \ngroup\n: 501,\n    \nhome\n: \n/home/devops\n,\n    \nname\n: \ndevops\n,\n    \nshell\n: \n/bin/bash\n,\n    \nstate\n: \npresent\n,\n    \nsystem\n: false,\n    \nuid\n: 501\n}\n192.168.61.12 | SUCCESS =\n {\n    \nchanged\n: true,\n    \ncomment\n: \n,\n    \ncreatehome\n: true,\n    \ngroup\n: 501,\n    \nhome\n: \n/home/devops\n,\n    \nname\n: \ndevops\n,\n    \nshell\n: \n/bin/bash\n,\n    \nstate\n: \npresent\n,\n    \nsystem\n: false,\n    \nuid\n: 501\n}\n\n\n\n\nCopy a file using \ncopy\n modules\n\n\nWe will copy file from control node to app servers.\n\n\nansible app -m copy -a \nsrc=/vagrant/test.txt dest=/tmp/test.txt\n\n\n\n\n\nFile will be copied over to our app server machines...\n\n\n192.168.61.13 | SUCCESS =\n {\n    \nchanged\n: true,\n    \nchecksum\n: \n3160f8f941c330444aac253a9e6420cd1a65bfe2\n,\n    \ndest\n: \n/tmp/test.txt\n,\n    \ngid\n: 500,\n    \ngroup\n: \nvagrant\n,\n    \nmd5sum\n: \n9052de4cff7e8a18de586f785e711b97\n,\n    \nmode\n: \n0664\n,\n    \nowner\n: \nvagrant\n,\n    \nsize\n: 11,\n    \nsrc\n: \n/home/vagrant/.ansible/tmp/ansible-tmp-1472991990.29-63683023616899/source\n,\n    \nstate\n: \nfile\n,\n    \nuid\n: 500\n}\n192.168.61.12 | SUCCESS =\n {\n    \nchanged\n: true,\n    \nchecksum\n: \n3160f8f941c330444aac253a9e6420cd1a65bfe2\n,\n    \ndest\n: \n/tmp/test.txt\n,\n    \ngid\n: 500,\n    \ngroup\n: \nvagrant\n,\n    \nmd5sum\n: \n9052de4cff7e8a18de586f785e711b97\n,\n    \nmode\n: \n0664\n,\n    \nowner\n: \nvagrant\n,\n    \nsize\n: 11,\n    \nsrc\n: \n/home/vagrant/.ansible/tmp/ansible-tmp-1472991990.26-218089785548663/source\n,\n    \nstate\n: \nfile\n,\n    \nuid\n: 500\n}\n\n\n\n\n\nExercises:\n\n\n\n\nAdd another system group (not inventory group) called \nlb\n in inventory with respective host ip\n\n\nAdd a system user called \njoe\n on all  app servers. Make sure that the user has a home directory\n\n\nInstall  package \nvim\n using the correct \nAd-Hoc\n command\n\n\nExamine all the available module\nhttp://docs.ansible.com/ansible/modules_by_category.html\n\n\nFind out the difference between the \ncommand\n module and \nshell\n module. Try running the following command with both these modules,\n\n\n\n\nfree | grep -i swap\n\n\n\n\n\n\n\nUse command module to show \nuptime\n on the host  \n\n\nInstall docker-engine using the yum/apt module\n\n\nUsing docker-image module, pull \nhello-world\n image on web server", 
            "title": "Ad Hoc Server Maangement with Ansible"
        }, 
        {
            "location": "/ad-hoc/#getting-started-with-ansible-ad-hoc-server-management", 
            "text": "", 
            "title": "Getting Started with Ansible (Ad Hoc Server Management)"
        }, 
        {
            "location": "/ad-hoc/#creating-project-specific-ansible-configuration", 
            "text": "The default configurations for ansible resides at /etc/ansible/ansible.cfg. Instead of relying on defaults, we are going to creates  a custom configuration file for our project. The advantage with that is we could take this configurations on any host and execute it the same way, without touching the default system configurations.  This custom configurations will essentially  override the values in /etc/ansible/ansible/cfg.", 
            "title": "Creating Project Specific Ansible Configuration"
        }, 
        {
            "location": "/ad-hoc/#ansible-configuration-file", 
            "text": "Change into /vagrant/code/chap3 directory on your ansible host. Create a file called ansible.cfg  Add  the following contents to the file.  On Ansible Control node,  cd chap4  Create  ansible.cfg  in chap4  [defaults]\nremote_user = devops\ninventory   = environments/prod\nretry_files_save_path = /tmp\nhost_key_checking = False\nlog_path=~/ansible.log", 
            "title": "Ansible configuration file"
        }, 
        {
            "location": "/ad-hoc/#creating-host-inventory", 
            "text": "Since you are going to create a environment specific inventory,\ncreate a  environments  directory and a  file  inside it called  prod  mkdir environments  Create inventory file  file: environments/prod  Let's create three groups as follows,  [local]\nlocalhost ansible_connection=local\n\n[lb]\nlb\n\n[app]\napp1\napp2\n\n\n[db]\ndb\n\n[prod:children]\nlb\napp\ndb   First group contains the localhost, the control host. Since it does not need to be connected over ssh, it mandates we add ansible_connection=local option  Second group contains  Application Servers. We will add  two app servers to this group.  Third group holds the information about the database servers.   The inventory file should look like below.", 
            "title": "Creating Host Inventory"
        }, 
        {
            "location": "/ad-hoc/#ansible-ping", 
            "text": "We will use Ansible to make sure all the hosts are reachable  ansible all -m ping  [Output]  lb | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\napp1 | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\napp2 | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\ndb | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}", 
            "title": "Ansible ping"
        }, 
        {
            "location": "/ad-hoc/#ad-hoc-commands", 
            "text": "Try running following  fire-and-forget  Ad-Hoc commands...", 
            "title": "Ad Hoc commands"
        }, 
        {
            "location": "/ad-hoc/#run-hostname-command-on-all-hosts", 
            "text": "Let us print the hostname of all the hosts  ansible all -a hostname  [output]  localhost | SUCCESS | rc=0  \nansible\n\n192.168.61.11 | SUCCESS | rc=0  \ndb\n\n192.168.61.12 | SUCCESS | rc=0  \napp\n\n192.168.61.13 | SUCCESS | rc=0  \napp", 
            "title": "Run hostname command on all hosts"
        }, 
        {
            "location": "/ad-hoc/#check-the-uptime", 
            "text": "How long the hosts are  up ?  ansible all -a uptime  [Output]  localhost | SUCCESS | rc=0  \n 13:17:13 up  2:21,  1 user,  load average: 0.16, 0.03, 0.01\n\n192.168.61.12 | SUCCESS | rc=0  \n 13:17:14 up  1:50,  2 users,  load average: 0.00, 0.00, 0.00\n\n192.168.61.13 | SUCCESS | rc=0  \n 13:17:14 up  1:47,  2 users,  load average: 0.00, 0.00, 0.00\n\n192.168.61.11 | SUCCESS | rc=0  \n 13:17:14 up  1:36,  2 users,  load average: 0.00, 0.00, 0.00", 
            "title": "Check the uptime"
        }, 
        {
            "location": "/ad-hoc/#check-memory-info-on-app-servers", 
            "text": "Does my app servers have any disk space  free ?  ansible app -a free  [Output]  192.168.61.13 | SUCCESS | rc=0  \n             total       used       free     shared    buffers     cached\nMem:        372916     121480     251436        776      11160      46304\n-/+ buffers/cache:      64016     308900\nSwap:      4128764          0    4128764\n\n192.168.61.12 | SUCCESS | rc=0  \n             total       used       free     shared    buffers     cached\nMem:        372916     121984     250932        776      11228      46336\n-/+ buffers/cache:      64420     308496\nSwap:      4128764          0    4128764", 
            "title": "Check memory info on app servers"
        }, 
        {
            "location": "/ad-hoc/#installing-packages", 
            "text": "Let us  install  Docker on app servers  ansible app -a  yum install -y docker-engine   This command will fail.  [Output]  192.168.61.13 | FAILED | rc=1  \nLoaded plugins: fastestmirror, prioritiesYou need to be root to perform this command.\n\n192.168.61.12 | FAILED | rc=1  \nLoaded plugins: fastestmirror, prioritiesYou need to be root to perform this command.  Run the fillowing command with sudo permissions.  ansible app -s -a  yum install -y docker-engine   This will install docker in our app servers  [Output]  192.168.61.12 | SUCCESS | rc=0  \nLoaded plugins: fastestmirror, priorities\nSetting up Install Process\nLoading mirror speeds from cached hostfile\n * base: mirrors.nhanhoa.com\n * epel: mirror.rise.ph\n * extras: mirror.fibergrid.in\n * updates: mirror.fibergrid.in\n283 packages excluded due to repository priority protections\nResolving Dependencies\n--  Running transaction check\n---  Package docker-engine.x86_64 0:1.7.1-1.el6 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version              Repository          Size\n================================================================================\nInstalling:\n docker-engine       x86_64       1.7.1-1.el6          local_docker       4.5 M\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 4.5 M\nInstalled size: 19 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : docker-engine-1.7.1-1.el6.x86_64                             1/1\n  Verifying  : docker-engine-1.7.1-1.el6.x86_64                             1/1\n\nInstalled:\n  docker-engine.x86_64 0:1.7.1-1.el6\n\nComplete!\n\n192.168.61.13 | SUCCESS | rc=0  \nLoaded plugins: fastestmirror, priorities\nSetting up Install Process\nLoading mirror speeds from cached hostfile\n * base: mirror.fibergrid.in\n * epel: mirror.rise.ph\n * extras: mirror.fibergrid.in\n * updates: mirror.fibergrid.in\n283 packages excluded due to repository priority protections\nResolving Dependencies\n--  Running transaction check\n---  Package docker-engine.x86_64 0:1.7.1-1.el6 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package             Arch         Version              Repository          Size\n================================================================================\nInstalling:\n docker-engine       x86_64       1.7.1-1.el6          local_docker       4.5 M\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 4.5 M\nInstalled size: 19 M\nDownloading Packages:\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : docker-engine-1.7.1-1.el6.x86_64                             1/1\n  Verifying  : docker-engine-1.7.1-1.el6.x86_64                             1/1\n\nInstalled:\n  docker-engine.x86_64 0:1.7.1-1.el6\n\nComplete!", 
            "title": "Installing packages"
        }, 
        {
            "location": "/ad-hoc/#running-commands-one-machine-at-a-time", 
            "text": "Do you want a command to run on  one machine at a time  ?  ansible all -f 1 -a  free", 
            "title": "Running commands one machine at a time"
        }, 
        {
            "location": "/ad-hoc/#using-modules-to-manage-the-state-of-infrastructure", 
            "text": "", 
            "title": "Using modules to manage the state of infrastructure"
        }, 
        {
            "location": "/ad-hoc/#creating-users-and-groups-using-user-and-group", 
            "text": "To create a group  ansible app -s -m group -a  name=admin state=present   The output will be,  192.168.61.13 | SUCCESS =  {\n     changed : true,\n     gid : 501,\n     name :  admin ,\n     state :  present ,\n     system : false\n}\n192.168.61.12 | SUCCESS =  {\n     changed : true,\n     gid : 501,\n     name :  admin ,\n     state :  present ,\n     system : false\n}  To create a user  ansible app -s -m user -a  name=devops group=admin createhome=yes   This will create user  devops ,  192.168.61.13 | SUCCESS =  {\n     changed : true,\n     comment :  ,\n     createhome : true,\n     group : 501,\n     home :  /home/devops ,\n     name :  devops ,\n     shell :  /bin/bash ,\n     state :  present ,\n     system : false,\n     uid : 501\n}\n192.168.61.12 | SUCCESS =  {\n     changed : true,\n     comment :  ,\n     createhome : true,\n     group : 501,\n     home :  /home/devops ,\n     name :  devops ,\n     shell :  /bin/bash ,\n     state :  present ,\n     system : false,\n     uid : 501\n}", 
            "title": "Creating users and groups using user and group"
        }, 
        {
            "location": "/ad-hoc/#copy-a-file-using-copy-modules", 
            "text": "We will copy file from control node to app servers.  ansible app -m copy -a  src=/vagrant/test.txt dest=/tmp/test.txt   File will be copied over to our app server machines...  192.168.61.13 | SUCCESS =  {\n     changed : true,\n     checksum :  3160f8f941c330444aac253a9e6420cd1a65bfe2 ,\n     dest :  /tmp/test.txt ,\n     gid : 500,\n     group :  vagrant ,\n     md5sum :  9052de4cff7e8a18de586f785e711b97 ,\n     mode :  0664 ,\n     owner :  vagrant ,\n     size : 11,\n     src :  /home/vagrant/.ansible/tmp/ansible-tmp-1472991990.29-63683023616899/source ,\n     state :  file ,\n     uid : 500\n}\n192.168.61.12 | SUCCESS =  {\n     changed : true,\n     checksum :  3160f8f941c330444aac253a9e6420cd1a65bfe2 ,\n     dest :  /tmp/test.txt ,\n     gid : 500,\n     group :  vagrant ,\n     md5sum :  9052de4cff7e8a18de586f785e711b97 ,\n     mode :  0664 ,\n     owner :  vagrant ,\n     size : 11,\n     src :  /home/vagrant/.ansible/tmp/ansible-tmp-1472991990.26-218089785548663/source ,\n     state :  file ,\n     uid : 500\n}", 
            "title": "Copy a file using copy modules"
        }, 
        {
            "location": "/ad-hoc/#exercises", 
            "text": "Add another system group (not inventory group) called  lb  in inventory with respective host ip  Add a system user called  joe  on all  app servers. Make sure that the user has a home directory  Install  package  vim  using the correct  Ad-Hoc  command  Examine all the available module\nhttp://docs.ansible.com/ansible/modules_by_category.html  Find out the difference between the  command  module and  shell  module. Try running the following command with both these modules,   free | grep -i swap    Use command module to show  uptime  on the host    Install docker-engine using the yum/apt module  Using docker-image module, pull  hello-world  image on web server", 
            "title": "Exercises:"
        }, 
        {
            "location": "/playbooks/", 
            "text": "Writing Playbook for Base System Configurations\n\n\nIn this tutorial we are going to create a simple playbook to add system users, install and start ntp service and some basic utilities.\n\n\nProblem Statement\n  \n\n\nYou have to create a playbook to configure all linux  systems  which will  \n\n\n\n\ncreate a admin user with uid 5001\n\n\nremove  user dojo\n\n\ninstall tree  utility\n\n\ninstall ntp\n\n\n\n\non all systems which belong to  \nprod\n group in the inventory\n\n\nTo prepare for this chapter, lets switch the directory in the workspace,\n\n\n\n\nFrom the workspace, change to \nchap5\n\n\n\n\ncd  chap5\n\n\n\n\n\n\n\n\nEdit \nenvironments/prod\n if required and comment the hosts which are absent.\n\n\n\n\n\n\nCreate a new file with name \nsystems.yml\n and add the following content to it\n\n\n\n\n\n\n---\n  - name: Base Configurations for ALL hosts\n    hosts: all\n    become: true\n    tasks:\n      - name: create admin user\n        user: name=admin state=present uid=5001\n\n      - name: remove dojo\n        user: name=dojo  state=absent\n\n      - name: install tree\n        yum:  name=tree  state=present\n\n      - name: install ntp\n        yum:  name=ntp   state=present\n\n\n\n\n\nValidating Syntax\n\n\nOption 1 : Using --syntax-check option with ansible-playbook\n\n\nansible-playbook systems.yml --syntax-check\n\n\n\n\nExercise:\n   Break the syntax, run playbook with --syntax check again, and learn how it works.\n\n\nOption 2 : Using YAML Linter Online\n\n\nAnother way to validate syntax\n\n\n\n\nVisit http://www.yamllinter.com\n    \n\n\n\n\nUsing ansible-playbook utility\n\n\nWe will start using ansible-playbook utility to execute playbooks.\n\n\nTo learn how to use ansible-playbook execute the following command,\n\n\n  ansible-playbook --help\n\n\n\n\n\n[output]\n\nUsage: ansible-playbook systems.yml\n\nOptions:\n  --ask-become-pass     ask for privilege escalation password\n  -k, --ask-pass        ask for connection password\n  --ask-su-pass         ask for su password (deprecated, use become)\n  -K, --ask-sudo-pass   ask for sudo password (deprecated, use become)\n  --ask-vault-pass      ask for vault password\n  -b, --become          run operations with become (nopasswd implied)\n  --become-method=BECOME_METHOD\n                        privilege escalation method to use (default=sudo),\n                        valid choices: [ sudo | su | pbrun | pfexec | runas |\n                        doas ]\n\n.......\n\n\n\n\nDry Run\n\n\nTo execute ansible in a check mode, which will simulate tasks on the remote nodes, without actually committing, ansible provides --check or -C option. This can be invoked as ,\n\n\nansible-playbook systems.yml --check\n\n\n\n\n\nor\n\n\nansible-playbook systems.yml -C\n\n\n\n\nListing Hosts, Tasks and Tags in a Playbook\n\n\nansible-playbook systems.yml --list-hosts\n\nansible-playbook systems.yml --list-tasks\n\nansible-playbook systems.yml --list-tags\n\n\n\n\nExecuting Actions with Playbooks\n\n\nTo execute  the playbook, we are going to execute \nansible-playbook\n comman with playbook  YAML file as an argument. Since we have already defined the inventory and configurations, additional options are not necessary at this time.\n\n\nansible-playbook systems.yml\n\n\n\n\n[output]\n\nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.14]\nok: [192.168.61.11]\nok: [localhost]\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [create admin user] *******************************************************\nchanged: [192.168.61.13]\nchanged: [192.168.61.12]\nchanged: [localhost]\nchanged: [192.168.61.11]\nchanged: [192.168.61.14]\n\nTASK [remove dojo] *************************************************************\nchanged: [192.168.61.14]\nchanged: [localhost]\nchanged: [192.168.61.12]\nchanged: [192.168.61.11]\nchanged: [192.168.61.13]\n\nTASK [install tree] ************************************************************\nok: [localhost]\nok: [192.168.61.13]\nok: [192.168.61.12]\nok: [192.168.61.14]\nok: [192.168.61.11]\n\nTASK [install ntp] *************************************************************\nchanged: [192.168.61.12]\nchanged: [192.168.61.13]\nchanged: [192.168.61.11]\nchanged: [localhost]\nchanged: [192.168.61.14]\n\n\n\n\n\nError Handling and Debugging\n\n\nWe are now going to add a new task to the playbook that we created. This task would start ntp service on all prod hosts.\n\n\nWhen you add this task, make sure the indentation is correct.\n\n\n\n      - name: start ntp service\n        service: name=ntp state=started enabled=yes\n\n\n\n\n\n\n\nApply playbook again, check the output\n\n\n\n\nansible-playbook systems.yml\n\n\n\n\n\n[output]\n\n\nTASK [start ntp service] *******************************************************\nfatal: [localhost]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nno service or tool found for: ntp\n}\nfatal: [192.168.61.11]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nno service or tool found for: ntp\n}\nfatal: [192.168.61.12]: FAILED! =\n {\nchanged\n: false, \nfailed\n: true, \nmsg\n: \nno service or tool found for: ntp\n}\n\nNO MORE HOSTS LEFT *************************************************************\n    to retry, use: --limit @/tmp/playbook.retry\n\nPLAY RECAP *********************************************************************\n192.168.61.11              : ok=5    changed=0    unreachable=0    failed=1\n192.168.61.12              : ok=5    changed=0    unreachable=0    failed=1\nlocalhost                  : ok=5    changed=0    unreachable=0    failed=1\n\n\n\n\nExercise : There was a intentional error introduced in the code. Identify the error from the log message above, correct it  and run the playbook again. This time you should run it  only on the failed hosts by limiting  with the retry file mentioned above (e.g. --limit @/tmp/playbook.retry )\n\n\nDebugging Technique : Step By Step Execution\n\n\nAnsible provides a way to execute tasks step by step, asking you whether to run or skip each task. This can be useful while debugging issues.\n\n\nansible-playbook systems.yml --step\n\n\n\n\n[Output]\n\n\nroot@control:/workspace/chap5# ansible-playbook systems.yml --step                             \n\nPLAY [Base Configurations for ALL hosts] ***************************************                \nPerform task: TASK: setup (N)o/(y)es/(c)ontinue: y                                              \n\n\nTASK [setup] *******************************************************************                \ny                                                                                               \nok: [app1]                                                                                      \nok: [db]                                                                                        \nok: [app2]                                                                                      \nok: [lb]                                                                                        \nPerform task: TASK: create admin user (N)o/(y)es/(c)ontinue:                                    \n\nTASK [create admin user] *******************************************************                \nyok: [app2]                                                                                     \nok: [app1]                                                                                      \nok: [db]                                                                                        \nok: [lb]                                                                                        \n\nPerform task: TASK: install tree (N)o/(y)es/(c)ontinue: y                                       \n\nTASK [install tree] ************************************************************                \nok: [app2]                                                                                      \nok: [lb]                                                                                        \nok: [app1]                                                                                      \nok: [db]                                                  \n\n\n\n\nAdding Additional  Play\n\n\nProblem Statement:\n\n\nYou have to add a  new play to configure the following only on the app servers\n\n\n\n\ncreate a deploy user with uid 5003\n\n\ninstall git\n\n\non all app servers in the inventory\n\n\n\n\nLets add a second play specific to app servers. Add the following block of code in systems.yml file and save   \n\n\n- name: App Server Configurations\n  hosts: app\n  become: true\n  tasks:\n    - name: create deploy user\n      user: name=deploy state=present uid=5003\n\n    - name: install git\n      yum:  name=git  state=present\n\n\n\n\nRun the playbook again...  \n\n\nansible-playbook systems.yml\n\n\n\n\n.......\n\nPLAY [App Server Configurations] ***********************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [create app user] *********************************************************\nchanged: [192.168.61.12]\nchanged: [192.168.61.13]\n\nTASK [install git] *************************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nPLAY RECAP *********************************************************************\n192.168.61.11              : ok=6    changed=0    unreachable=0    failed=0\n192.168.61.12              : ok=9    changed=1    unreachable=0    failed=0\n192.168.61.13              : ok=9    changed=1    unreachable=0    failed=0\n192.168.61.14              : ok=6    changed=0    unreachable=0    failed=0\nlocalhost                  : ok=6    changed=0    unreachable=0    failed=0\n\n\n\n\nLimiting the execution to a particular group\n\n\nNow run the following command to restrict the playbook execution to \napp servers\n  \n\n\nansible-playbook systems.yml --limit app\n\n\n\n\nThis will give us the following output, plays will be executed only on app servers...  \n\n\n\nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\n.........\n\nTASK [start ntp service] *******************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nPLAY [App Server Configurations] ***********************************************\n\n........\n\nTASK [install git] *************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nPLAY RECAP *********************************************************************\n192.168.61.12              : ok=9    changed=0    unreachable=0    failed=0\n192.168.61.13              : ok=9    changed=0    unreachable=0    failed=0\n\n\n\n\n\nExercises:\n\n\nNano Project\n: Create a Playbook with the following specifications,\n\n\n\n\nIt should apply only on local host (ansible host)\n\n\nShould use become method\n\n\nShould create a \nuser\n called webadmin with shell as \"/bin/sh\"\n\n\nShould install and start \nnginx\n service\n\n\nShould \ndeploy\n a sample html app into the default web root directory of nginx using ansible's \ngit\n module.\n\n\nSource repo:  https://github.com/schoolofdevops/html-sample-app\n\n\nDeploy Path : /usr/share/nginx/html/app\n\n\n\n\n\n\nOnce deployed, validate the site by visting http://CONTROL_HOST_IP/app\n\n\n\n\nExercise\n: Disable Facts Gathering\n\n\n\n\nRun ansible playbook and observe the output\n\n\nAdd the following configuration parameter to ansible.cfg\n\n\n\n\ngathering = explicit\n\n\n\n\n\n\nLaunch ansible playbook run again, observe the output and compare it with the previous run.", 
            "title": "Writing Playbook for Base System Configurations"
        }, 
        {
            "location": "/playbooks/#writing-playbook-for-base-system-configurations", 
            "text": "In this tutorial we are going to create a simple playbook to add system users, install and start ntp service and some basic utilities.  Problem Statement     You have to create a playbook to configure all linux  systems  which will     create a admin user with uid 5001  remove  user dojo  install tree  utility  install ntp   on all systems which belong to   prod  group in the inventory  To prepare for this chapter, lets switch the directory in the workspace,   From the workspace, change to  chap5   cd  chap5    Edit  environments/prod  if required and comment the hosts which are absent.    Create a new file with name  systems.yml  and add the following content to it    ---\n  - name: Base Configurations for ALL hosts\n    hosts: all\n    become: true\n    tasks:\n      - name: create admin user\n        user: name=admin state=present uid=5001\n\n      - name: remove dojo\n        user: name=dojo  state=absent\n\n      - name: install tree\n        yum:  name=tree  state=present\n\n      - name: install ntp\n        yum:  name=ntp   state=present", 
            "title": "Writing Playbook for Base System Configurations"
        }, 
        {
            "location": "/playbooks/#validating-syntax", 
            "text": "Option 1 : Using --syntax-check option with ansible-playbook  ansible-playbook systems.yml --syntax-check  Exercise:    Break the syntax, run playbook with --syntax check again, and learn how it works.  Option 2 : Using YAML Linter Online  Another way to validate syntax   Visit http://www.yamllinter.com", 
            "title": "Validating Syntax"
        }, 
        {
            "location": "/playbooks/#using-ansible-playbook-utility", 
            "text": "We will start using ansible-playbook utility to execute playbooks.  To learn how to use ansible-playbook execute the following command,    ansible-playbook --help  [output]\n\nUsage: ansible-playbook systems.yml\n\nOptions:\n  --ask-become-pass     ask for privilege escalation password\n  -k, --ask-pass        ask for connection password\n  --ask-su-pass         ask for su password (deprecated, use become)\n  -K, --ask-sudo-pass   ask for sudo password (deprecated, use become)\n  --ask-vault-pass      ask for vault password\n  -b, --become          run operations with become (nopasswd implied)\n  --become-method=BECOME_METHOD\n                        privilege escalation method to use (default=sudo),\n                        valid choices: [ sudo | su | pbrun | pfexec | runas |\n                        doas ]\n\n.......", 
            "title": "Using ansible-playbook utility"
        }, 
        {
            "location": "/playbooks/#dry-run", 
            "text": "To execute ansible in a check mode, which will simulate tasks on the remote nodes, without actually committing, ansible provides --check or -C option. This can be invoked as ,  ansible-playbook systems.yml --check  or  ansible-playbook systems.yml -C", 
            "title": "Dry Run"
        }, 
        {
            "location": "/playbooks/#listing-hosts-tasks-and-tags-in-a-playbook", 
            "text": "ansible-playbook systems.yml --list-hosts\n\nansible-playbook systems.yml --list-tasks\n\nansible-playbook systems.yml --list-tags", 
            "title": "Listing Hosts, Tasks and Tags in a Playbook"
        }, 
        {
            "location": "/playbooks/#executing-actions-with-playbooks", 
            "text": "To execute  the playbook, we are going to execute  ansible-playbook  comman with playbook  YAML file as an argument. Since we have already defined the inventory and configurations, additional options are not necessary at this time.  ansible-playbook systems.yml  [output]\n\nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.14]\nok: [192.168.61.11]\nok: [localhost]\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [create admin user] *******************************************************\nchanged: [192.168.61.13]\nchanged: [192.168.61.12]\nchanged: [localhost]\nchanged: [192.168.61.11]\nchanged: [192.168.61.14]\n\nTASK [remove dojo] *************************************************************\nchanged: [192.168.61.14]\nchanged: [localhost]\nchanged: [192.168.61.12]\nchanged: [192.168.61.11]\nchanged: [192.168.61.13]\n\nTASK [install tree] ************************************************************\nok: [localhost]\nok: [192.168.61.13]\nok: [192.168.61.12]\nok: [192.168.61.14]\nok: [192.168.61.11]\n\nTASK [install ntp] *************************************************************\nchanged: [192.168.61.12]\nchanged: [192.168.61.13]\nchanged: [192.168.61.11]\nchanged: [localhost]\nchanged: [192.168.61.14]", 
            "title": "Executing Actions with Playbooks"
        }, 
        {
            "location": "/playbooks/#error-handling-and-debugging", 
            "text": "We are now going to add a new task to the playbook that we created. This task would start ntp service on all prod hosts.  When you add this task, make sure the indentation is correct.  \n      - name: start ntp service\n        service: name=ntp state=started enabled=yes   Apply playbook again, check the output   ansible-playbook systems.yml  [output]  TASK [start ntp service] *******************************************************\nfatal: [localhost]: FAILED! =  { changed : false,  failed : true,  msg :  no service or tool found for: ntp }\nfatal: [192.168.61.11]: FAILED! =  { changed : false,  failed : true,  msg :  no service or tool found for: ntp }\nfatal: [192.168.61.12]: FAILED! =  { changed : false,  failed : true,  msg :  no service or tool found for: ntp }\n\nNO MORE HOSTS LEFT *************************************************************\n    to retry, use: --limit @/tmp/playbook.retry\n\nPLAY RECAP *********************************************************************\n192.168.61.11              : ok=5    changed=0    unreachable=0    failed=1\n192.168.61.12              : ok=5    changed=0    unreachable=0    failed=1\nlocalhost                  : ok=5    changed=0    unreachable=0    failed=1  Exercise : There was a intentional error introduced in the code. Identify the error from the log message above, correct it  and run the playbook again. This time you should run it  only on the failed hosts by limiting  with the retry file mentioned above (e.g. --limit @/tmp/playbook.retry )", 
            "title": "Error Handling and Debugging"
        }, 
        {
            "location": "/playbooks/#debugging-technique-step-by-step-execution", 
            "text": "Ansible provides a way to execute tasks step by step, asking you whether to run or skip each task. This can be useful while debugging issues.  ansible-playbook systems.yml --step  [Output]  root@control:/workspace/chap5# ansible-playbook systems.yml --step                             \n\nPLAY [Base Configurations for ALL hosts] ***************************************                \nPerform task: TASK: setup (N)o/(y)es/(c)ontinue: y                                              \n\n\nTASK [setup] *******************************************************************                \ny                                                                                               \nok: [app1]                                                                                      \nok: [db]                                                                                        \nok: [app2]                                                                                      \nok: [lb]                                                                                        \nPerform task: TASK: create admin user (N)o/(y)es/(c)ontinue:                                    \n\nTASK [create admin user] *******************************************************                \nyok: [app2]                                                                                     \nok: [app1]                                                                                      \nok: [db]                                                                                        \nok: [lb]                                                                                        \n\nPerform task: TASK: install tree (N)o/(y)es/(c)ontinue: y                                       \n\nTASK [install tree] ************************************************************                \nok: [app2]                                                                                      \nok: [lb]                                                                                        \nok: [app1]                                                                                      \nok: [db]", 
            "title": "Debugging Technique : Step By Step Execution"
        }, 
        {
            "location": "/playbooks/#adding-additional-play", 
            "text": "Problem Statement:  You have to add a  new play to configure the following only on the app servers   create a deploy user with uid 5003  install git  on all app servers in the inventory   Lets add a second play specific to app servers. Add the following block of code in systems.yml file and save     - name: App Server Configurations\n  hosts: app\n  become: true\n  tasks:\n    - name: create deploy user\n      user: name=deploy state=present uid=5003\n\n    - name: install git\n      yum:  name=git  state=present  Run the playbook again...    ansible-playbook systems.yml  .......\n\nPLAY [App Server Configurations] ***********************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [create app user] *********************************************************\nchanged: [192.168.61.12]\nchanged: [192.168.61.13]\n\nTASK [install git] *************************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nPLAY RECAP *********************************************************************\n192.168.61.11              : ok=6    changed=0    unreachable=0    failed=0\n192.168.61.12              : ok=9    changed=1    unreachable=0    failed=0\n192.168.61.13              : ok=9    changed=1    unreachable=0    failed=0\n192.168.61.14              : ok=6    changed=0    unreachable=0    failed=0\nlocalhost                  : ok=6    changed=0    unreachable=0    failed=0", 
            "title": "Adding Additional  Play"
        }, 
        {
            "location": "/playbooks/#limiting-the-execution-to-a-particular-group", 
            "text": "Now run the following command to restrict the playbook execution to  app servers     ansible-playbook systems.yml --limit app  This will give us the following output, plays will be executed only on app servers...    \nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\n.........\n\nTASK [start ntp service] *******************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nPLAY [App Server Configurations] ***********************************************\n\n........\n\nTASK [install git] *************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nPLAY RECAP *********************************************************************\n192.168.61.12              : ok=9    changed=0    unreachable=0    failed=0\n192.168.61.13              : ok=9    changed=0    unreachable=0    failed=0", 
            "title": "Limiting the execution to a particular group"
        }, 
        {
            "location": "/playbooks/#exercises", 
            "text": "", 
            "title": "Exercises:"
        }, 
        {
            "location": "/playbooks/#nano-project-create-a-playbook-with-the-following-specifications", 
            "text": "It should apply only on local host (ansible host)  Should use become method  Should create a  user  called webadmin with shell as \"/bin/sh\"  Should install and start  nginx  service  Should  deploy  a sample html app into the default web root directory of nginx using ansible's  git  module.  Source repo:  https://github.com/schoolofdevops/html-sample-app  Deploy Path : /usr/share/nginx/html/app    Once deployed, validate the site by visting http://CONTROL_HOST_IP/app", 
            "title": "Nano Project: Create a Playbook with the following specifications,"
        }, 
        {
            "location": "/playbooks/#exercise-disable-facts-gathering", 
            "text": "Run ansible playbook and observe the output  Add the following configuration parameter to ansible.cfg   gathering = explicit   Launch ansible playbook run again, observe the output and compare it with the previous run.", 
            "title": "Exercise: Disable Facts Gathering"
        }, 
        {
            "location": "/roles/", 
            "text": "Configuring app server environment with Roles\n\n\nIn the previous chapter, you have created and applied playbook for base systems configurations. Now is the time to start creating modular, reusable library of code for application configurations. In this chapter, we are going to write such modular code, in the form of roles and setup application server.  \n\n\nWe are going to create the roles with following specs,\n\n\napache role which will\n\n\n\n\nInstall \nhttpd\n package\n\n\nconfigure \nhttpd.conf\n\n\nStart \nhttpd\n service\n\n\nAdd a \nhandler\n to restart service\n\n\n\n\nphp\n role to\n\n\n\n\ninstall \nphp\n and \nphp-mysql\n\n\nrestart apache when packages are installed\n\n\n\n\nWe will also refactor  \nsystems.yml\n and move all the tasks to its own role i.e. \nsystems\n\n\nCreating Role Scaffolding for Apache\n\n\n\n\nChange working  directory to \nchap6\n\n\n\n\ncd  chap6\n\n\n\n\n\n\nCreate roles directory\n\n\n\n\nmkdir roles\n\n\n\n\n\n\nGenerate role scaffolding using ansible-galaxy\n\n\n\n\nansible-galaxy init --offline --init-path=roles  apache\n\n\n\n\n\n\nValidate\n\n\n\n\ntree roles/\n\n\n\n\n[Output]\n\n\n  roles/\n    \u2514\u2500\u2500 apache\n        \u251c\u2500\u2500 defaults\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 files\n        \u251c\u2500\u2500 handlers\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 meta\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 README.md\n        \u251c\u2500\u2500 tasks\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 templates\n        \u251c\u2500\u2500 tests\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 inventory\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 test.yml\n        \u2514\u2500\u2500 vars\n            \u2514\u2500\u2500 main.yml\n\n\n\n\n\nWriting Tasks to Install and Start  Apache Web Service\n\n\nWe are going to create three different tasks files, one for each phase of application lifecycle\n  * Install\n  * Configure\n  * Start Service\n\n\nTo begin with, in this part, we will install and start apache.\n\n\n\n\nTo install apache, Create \nroles/apache/tasks/install.yml\n\n\n\n\n---\n  - name: install apache web server\n    yum:\n      name: httpd\n      state: installed\n\n\n\n\n\n\nTo start the service, create  \nroles/apache/tasks/service.yml\n with the following content  \n\n\n\n\n---\n  - name: start apache webserver\n    service:\n      name: httpd\n      state: started\n      enabled: true\n\n\n\n\nTo have these tasks being called, include them into main task.\n\n\n\n\nEdit roles/apache/tasks/main.yml\n\n\n\n\n---\n# tasks file for apache\n  - import_tasks: install.yml\n  - import_tasks: service.yml\n\n\n\n\nCreate and apply playbook to configure app servers\n\n\n\n\nCreate a playbook for app servers \napp.yml\n with following contents\n\n\n\n\n  ---\n  - hosts: app\n    become: true\n    roles:\n      - apache\n\n\n\n\n\n\nApply app.yml with ansible-playbook\n\n\n\n\n  ansible-playbook app.yml\n\n\n\n\n[Output]\n\n\nPLAY [Playbook to configure App Servers] *********************************************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [apache : Install Apache...] **********************************************\nchanged: [192.168.61.13]\nchanged: [192.168.61.12]\n\nTASK [apache : Starting Apache...] *********************************************\nchanged: [192.168.61.13]\nchanged: [192.168.61.12]\n\nPLAY RECAP *********************************************************************\n192.168.61.12              : ok=3    changed=2    unreachable=0    failed=0\n192.168.61.13              : ok=3    changed=2    unreachable=0    failed=0\n\n\n\n\nManaging Configurations\n\n\n\n\nCopy \nindex.html\n and \nhttpd.conf\n from \nchap6/helper\n to \n/roles/apache/files/\n directory    \n\n\n\n\n   cd chap6\n   cp helper/httpd.conf roles/apache/files/  \n\n\n\n\n\n\nCreate a task file at \nroles/apache/tasks/config.yml\n to copy the configuration file.    \n\n\n\n\n---\n  - name: copy over httpd configs\n    copy:\n      src: httpd.conf\n      dest: /etc/httpd.conf\n      owner: root\n      group: root\n      mode: 0644\n\n\n\n\n\nAdding Notifications and Handlers\n\n\n\n\nPreviously we have create a task in roles/apache/tasks/config.yml to copy over httpd.conf to the app server. Update this file to send a notification to restart  service on configuration update.  You simply have to add the line which starts with \nnotify\n\n\n\n\n---\n  - name: copy over httpd configs\n    copy:\n      src: httpd.conf\n      dest: /etc/httpd.conf\n      owner: root\n      group: root\n      mode: 0644\n    notify: Restart apache service\n\n\n\n\n\n\nCreate the notification handler by updating   \nroles/apache/handlers/main.yml\n  \n\n\n\n\n---\n  - name: Restart apache service\n    service: name=httpd state=restarted\n\n\n\n\nUpdate \ntasks/main.yml\n to call the newly created tasks file.\n\n\n---\n# tasks file for apache\n  - import_tasks: install.yml\n  - import_tasks: service.yml\n  - import_tasks: config.yml\n\n\n\n\nApply and validate if the configuration file is being copied and service restarted.\n\n\n ansible-playbook app.yml\n\n\n\n\nCreate a role to install php\n\n\nGenerate roles scaffold\n\n\nansible-galaxy init --offline --init-path=roles  php\n\n\n\n\nroles/php/tasks/install.yml\n\n\n---\n# install php related packages\n  - name: install php\n    package:\n      name: \n{{ item }}\n\n      state: installed\n    with_items:\n      - php\n      - php-mysql\n    notify: Restart apache service\n\n\n\n\nfile: roles/php/tasks/main.yml\n\n\n---\n# tasks file for php\n- import_tasks: install.yml\n\n\n\n\nUpdate \napp.yml\n playbook to invoke php role.\n\n\nfile: app.yml\n\n\n  ---\n  - hosts: app\n    become: true\n    roles:\n      - apache\n      - php\n\n\n\n\nApply the playbook\n\n\nansible-playbook app.yml\n\n\n\n\nSystems role, dependencies and nested roles\n\n\nYou have already written a playbook to define common systems configurations. Now, go ahead and refactor it so that instead of calling tasks from playbook itself, it goes into its own role, and then call on each server.\n\n\n\n\nCreate a base role with ansible-galaxy utility,  \n\n\n\n\n  ansible-galaxy init --offline --init-path=roles systems\n\n\n\n\n\n\nCopy over the  tasks from \nsystems.yml\n and lets just add it to   \n/roles/base/tasks/main.yml\n  \n\n\n\n\n---\n# tasks file for systems\n  - name: remove user dojo\n    user: \n\n      name=dojo\n      state=absent\n\n  - name: install tree utility\n    yum: \n\n      name=tree\n      state=present\n\n  - name: install ntp\n    yum: \n\n      name=ntp\n      state=installed\n\n\n\n\n\n\n\nDefine systems role as a dependency for  apache role,  \n\n\nUpdate meta data for Apache by editing \nroles/apache/meta/main.yml\n and adding the following\n\n\n\n\n---\ndependencies:\n - {role: systems}\n\n\n\n\nNext time you run  \napp.yml\n, observe if the above tasks get invoked as well.\n\n\nCreating a Site Wide Playbook\n\n\nWe will create a site wide playbook, which will call all the plays required to configure the complete infrastructure. Currently we have a single  playbook for App Servers. However, in future we would create many.\n\n\n\n\nCreate \nsite.yml\n in /vagrant/chap5 directory and add the following content\n\n\n\n\n  ---\n  # This is a sitewide playbook\n  # filename: site.yml\n  - import_playbook: app.yml\n\n\n\n\n\n\n\nExecute sitewide playbook as\n\n\n\n\nansible-playbook site.yml\n\n\n\n\n[Output]\n\n\n\nPLAY [Playbook to configure App Servers] ***************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : create admin user] ************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : remove dojo] ******************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : install tree] *****************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [base : install ntp] ******************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [base : start ntp service] ************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [apache : Installing Apache...] *******************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [apache : Starting Apache...] *********************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [apache : Copying configuration files...] *********************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [apache : Copying index.html file...] *************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nPLAY RECAP *********************************************************************\n192.168.61.12              : ok=10   changed=0    unreachable=0    failed=0\n192.168.61.13              : ok=10   changed=0    unreachable=0    failed=0\n\n\n\n\nExercises\n\n\nNano Project: Deploy a PHP Application\n\n\ndevops-demo-app is an application written in  PHP. You have already setup the environment above with apache and php roles, to deploy this application.  Your job is to write the ansible code to  deploy this application on app servers. This code will be in the form of a role.\n\n\nYou have been tasked to create a \nfroentend\n  role with the following specs,\n\n\n\n\nPull release packages from the github release page as provided in the resources below. Releases are in the form of an archive.\n\n\nMultiple copies of releases will be maintained on the app servers for enabling rollbacks. To support this, every time to deploy a new version of the app, create a new directory for it inside the /opt/app\n\n\n\n\ne.g.\n\nopt\n  | __ app\n         \\__ release\n                 |\n                 |____ devops-demo-app-1.0\n                 |\n                 |____ devops-demo-app-1.1\n\n\n\n\n\n\n\nCreate  a symlink from the current version path to  \n/var/www/html/app\n\n\n\n\ne.g.\n\n[root@app2 /]# ls -l /var/www/html/\ntotal 8\nlrwxrwxrwx 1 root root   36 Jan 16 13:28 app -\n /opt/app/release/devops-demo-app-1.1\n\n\n\n\n\nResources\n:\n\n\n\n\nPHP App Source\n : https://github.com/devopsdemoapps/devops-demo-app\n\n\nReleases\n: https://github.com/devopsdemoapps/devops-demo-app/releases\n\n\n\n\nOnce deployed, visiting  \n for app1 or with port 82 for (app2) should show the web app deployed.", 
            "title": "Configuring app server environment with Roles"
        }, 
        {
            "location": "/roles/#configuring-app-server-environment-with-roles", 
            "text": "In the previous chapter, you have created and applied playbook for base systems configurations. Now is the time to start creating modular, reusable library of code for application configurations. In this chapter, we are going to write such modular code, in the form of roles and setup application server.    We are going to create the roles with following specs,  apache role which will   Install  httpd  package  configure  httpd.conf  Start  httpd  service  Add a  handler  to restart service   php  role to   install  php  and  php-mysql  restart apache when packages are installed   We will also refactor   systems.yml  and move all the tasks to its own role i.e.  systems", 
            "title": "Configuring app server environment with Roles"
        }, 
        {
            "location": "/roles/#creating-role-scaffolding-for-apache", 
            "text": "Change working  directory to  chap6   cd  chap6   Create roles directory   mkdir roles   Generate role scaffolding using ansible-galaxy   ansible-galaxy init --offline --init-path=roles  apache   Validate   tree roles/  [Output]    roles/\n    \u2514\u2500\u2500 apache\n        \u251c\u2500\u2500 defaults\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 files\n        \u251c\u2500\u2500 handlers\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 meta\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 README.md\n        \u251c\u2500\u2500 tasks\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n        \u251c\u2500\u2500 templates\n        \u251c\u2500\u2500 tests\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 inventory\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 test.yml\n        \u2514\u2500\u2500 vars\n            \u2514\u2500\u2500 main.yml", 
            "title": "Creating Role Scaffolding for Apache"
        }, 
        {
            "location": "/roles/#writing-tasks-to-install-and-start-apache-web-service", 
            "text": "We are going to create three different tasks files, one for each phase of application lifecycle\n  * Install\n  * Configure\n  * Start Service  To begin with, in this part, we will install and start apache.   To install apache, Create  roles/apache/tasks/install.yml   ---\n  - name: install apache web server\n    yum:\n      name: httpd\n      state: installed   To start the service, create   roles/apache/tasks/service.yml  with the following content     ---\n  - name: start apache webserver\n    service:\n      name: httpd\n      state: started\n      enabled: true  To have these tasks being called, include them into main task.   Edit roles/apache/tasks/main.yml   ---\n# tasks file for apache\n  - import_tasks: install.yml\n  - import_tasks: service.yml", 
            "title": "Writing Tasks to Install and Start  Apache Web Service"
        }, 
        {
            "location": "/roles/#create-and-apply-playbook-to-configure-app-servers", 
            "text": "Create a playbook for app servers  app.yml  with following contents     ---\n  - hosts: app\n    become: true\n    roles:\n      - apache   Apply app.yml with ansible-playbook     ansible-playbook app.yml  [Output]  PLAY [Playbook to configure App Servers] *********************************************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [apache : Install Apache...] **********************************************\nchanged: [192.168.61.13]\nchanged: [192.168.61.12]\n\nTASK [apache : Starting Apache...] *********************************************\nchanged: [192.168.61.13]\nchanged: [192.168.61.12]\n\nPLAY RECAP *********************************************************************\n192.168.61.12              : ok=3    changed=2    unreachable=0    failed=0\n192.168.61.13              : ok=3    changed=2    unreachable=0    failed=0", 
            "title": "Create and apply playbook to configure app servers"
        }, 
        {
            "location": "/roles/#managing-configurations", 
            "text": "Copy  index.html  and  httpd.conf  from  chap6/helper  to  /roles/apache/files/  directory          cd chap6\n   cp helper/httpd.conf roles/apache/files/     Create a task file at  roles/apache/tasks/config.yml  to copy the configuration file.       ---\n  - name: copy over httpd configs\n    copy:\n      src: httpd.conf\n      dest: /etc/httpd.conf\n      owner: root\n      group: root\n      mode: 0644", 
            "title": "Managing Configurations"
        }, 
        {
            "location": "/roles/#adding-notifications-and-handlers", 
            "text": "Previously we have create a task in roles/apache/tasks/config.yml to copy over httpd.conf to the app server. Update this file to send a notification to restart  service on configuration update.  You simply have to add the line which starts with  notify   ---\n  - name: copy over httpd configs\n    copy:\n      src: httpd.conf\n      dest: /etc/httpd.conf\n      owner: root\n      group: root\n      mode: 0644\n    notify: Restart apache service   Create the notification handler by updating    roles/apache/handlers/main.yml      ---\n  - name: Restart apache service\n    service: name=httpd state=restarted  Update  tasks/main.yml  to call the newly created tasks file.  ---\n# tasks file for apache\n  - import_tasks: install.yml\n  - import_tasks: service.yml\n  - import_tasks: config.yml  Apply and validate if the configuration file is being copied and service restarted.   ansible-playbook app.yml", 
            "title": "Adding Notifications and Handlers"
        }, 
        {
            "location": "/roles/#create-a-role-to-install-php", 
            "text": "Generate roles scaffold  ansible-galaxy init --offline --init-path=roles  php  roles/php/tasks/install.yml  ---\n# install php related packages\n  - name: install php\n    package:\n      name:  {{ item }} \n      state: installed\n    with_items:\n      - php\n      - php-mysql\n    notify: Restart apache service  file: roles/php/tasks/main.yml  ---\n# tasks file for php\n- import_tasks: install.yml  Update  app.yml  playbook to invoke php role.  file: app.yml    ---\n  - hosts: app\n    become: true\n    roles:\n      - apache\n      - php  Apply the playbook  ansible-playbook app.yml", 
            "title": "Create a role to install php"
        }, 
        {
            "location": "/roles/#systems-role-dependencies-and-nested-roles", 
            "text": "You have already written a playbook to define common systems configurations. Now, go ahead and refactor it so that instead of calling tasks from playbook itself, it goes into its own role, and then call on each server.   Create a base role with ansible-galaxy utility,       ansible-galaxy init --offline --init-path=roles systems   Copy over the  tasks from  systems.yml  and lets just add it to    /roles/base/tasks/main.yml      ---\n# tasks file for systems\n  - name: remove user dojo\n    user:  \n      name=dojo\n      state=absent\n\n  - name: install tree utility\n    yum:  \n      name=tree\n      state=present\n\n  - name: install ntp\n    yum:  \n      name=ntp\n      state=installed   Define systems role as a dependency for  apache role,    Update meta data for Apache by editing  roles/apache/meta/main.yml  and adding the following   ---\ndependencies:\n - {role: systems}  Next time you run   app.yml , observe if the above tasks get invoked as well.", 
            "title": "Systems role, dependencies and nested roles"
        }, 
        {
            "location": "/roles/#creating-a-site-wide-playbook", 
            "text": "We will create a site wide playbook, which will call all the plays required to configure the complete infrastructure. Currently we have a single  playbook for App Servers. However, in future we would create many.   Create  site.yml  in /vagrant/chap5 directory and add the following content     ---\n  # This is a sitewide playbook\n  # filename: site.yml\n  - import_playbook: app.yml   Execute sitewide playbook as   ansible-playbook site.yml  [Output]  \nPLAY [Playbook to configure App Servers] ***************************************\n\nTASK [setup] *******************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : create admin user] ************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : remove dojo] ******************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : install tree] *****************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [base : install ntp] ******************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [base : start ntp service] ************************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [apache : Installing Apache...] *******************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [apache : Starting Apache...] *********************************************\nok: [192.168.61.13]\nok: [192.168.61.12]\n\nTASK [apache : Copying configuration files...] *********************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [apache : Copying index.html file...] *************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nPLAY RECAP *********************************************************************\n192.168.61.12              : ok=10   changed=0    unreachable=0    failed=0\n192.168.61.13              : ok=10   changed=0    unreachable=0    failed=0", 
            "title": "Creating a Site Wide Playbook"
        }, 
        {
            "location": "/roles/#exercises", 
            "text": "", 
            "title": "Exercises"
        }, 
        {
            "location": "/roles/#nano-project-deploy-a-php-application", 
            "text": "devops-demo-app is an application written in  PHP. You have already setup the environment above with apache and php roles, to deploy this application.  Your job is to write the ansible code to  deploy this application on app servers. This code will be in the form of a role.  You have been tasked to create a  froentend   role with the following specs,   Pull release packages from the github release page as provided in the resources below. Releases are in the form of an archive.  Multiple copies of releases will be maintained on the app servers for enabling rollbacks. To support this, every time to deploy a new version of the app, create a new directory for it inside the /opt/app   e.g.\n\nopt\n  | __ app\n         \\__ release\n                 |\n                 |____ devops-demo-app-1.0\n                 |\n                 |____ devops-demo-app-1.1   Create  a symlink from the current version path to   /var/www/html/app   e.g.\n\n[root@app2 /]# ls -l /var/www/html/\ntotal 8\nlrwxrwxrwx 1 root root   36 Jan 16 13:28 app -  /opt/app/release/devops-demo-app-1.1  Resources :   PHP App Source  : https://github.com/devopsdemoapps/devops-demo-app  Releases : https://github.com/devopsdemoapps/devops-demo-app/releases   Once deployed, visiting    for app1 or with port 82 for (app2) should show the web app deployed.", 
            "title": "Nano Project: Deploy a PHP Application"
        }, 
        {
            "location": "/templates_and_variables/", 
            "text": "Templates and Variables\n\n\nIn this  tutorial, we  are going to make the roles that we created earlier dynamically by adding templates and defining variables.\n\n\nVariables\n\n\nVariables are of two types\n\n\n\n\nAutomatic Variables/ Facts\n\n\nUser Defined Variables\n\n\n\n\nLets try to discover information about our systems by using facts.\n\n\nFinding Facts About Systems\n\n\n\n\nRun the following command to see to facts of db servers  \n\n\n\n\nansible db -m setup\n\n\n\n\n[Output]\n\n\n192.168.61.11 | SUCCESS =\n {\n        \nansible_facts\n: {\n            \nansible_all_ipv4_addresses\n: [\n                \n10.0.2.15\n,\n                \n192.168.61.11\n\n            ],\n            \nansible_all_ipv6_addresses\n: [\n                \nfe80::a00:27ff:fe30:3251\n,\n                \nfe80::a00:27ff:fe8e:83e0\n\n.....\n                \ntz_offset\n: \n+0100\n,\n                \nweekday\n: \nMonday\n,\n                \nweekday_number\n: \n1\n,\n                \nweeknumber\n: \n36\n,\n                \nyear\n: \n2016\n\n            }\n\n\n\n\nFiltering facts\n\n\n\n\nUse filter attribute to extract specific data\n\n\n\n\nansible db -m setup -a \nfilter=ansible_distribution\n\n\n\n\n\n[Output]\n\n\n192.168.61.11 | SUCCESS =\n {\n  \nansible_facts\n: {\n      \nansible_distribution\n: \nCentOS\n\n  },\n  \nchanged\n: false\n}  \n\n\n\n\nDefining release versions with vars\n\n\nCurrently, while deploying the application, the versions of the artifacts as well as release directories are defined statically. This should change to vars so that the version can be defined from one place, and dynamically so.\n\n\nDefine the default vars\n\n\nfile: roles/frontend/defaults/main.yml\n\n\n---\n# defaults file for frontend\n  app:\n    version: 1.5\n\n\n\n\n\nUpdate tasks to use the var defined above,\n\n\nfile: roles/frontend/tasks/main.yml\n\n\n- name: Download and extract the release\n  unarchive:\n    src: https://github.com/devopsdemoapps/devops-demo-app/archive/{{ app.version }}.tar.gz\n    dest: /opt/app/release\n    owner: apache\n    group: apache\n    creates: /opt/app/release/devops-demo-app-{{ app.version }}\n    remote_src: yes\n\n- name: create a symlink\n  file:\n    src: /opt/app/release/devops-demo-app-{{ app.version }}\n    dest: /var/www/html/app\n    owner: apache\n    group: apache\n    state: link\n\n\n\n\nTry This\n:\n  * Run playbook and check whether the above code works\n  * Change the version e.g. 1.4 and check if it has any effect\n\n\nCreating  application configurations dynamically\n\n\nApplication configs are defined with \nconfig.ini\n file.  The version of config.ini as shipped with the application is as follows,\n\n\n[database]\nhostname = DBHOST\nusername = SQLUSER\npassword = SQLPASSWORD\ndbname = SQLDBNAME\n\n[environment]\nenvironment = ENVNAME\n\n[prefs]\ncolor  = white\nfruit  = apple\ncar    = fiat\nlaptop = dell\n\n\n\n\nYou should be able to customize these configs. In order to do that, you need to do split this into 2 things as follows,\n\n\n\n\nvars\n which define the actual properties and allow you to change it from different places\n\n\na \njinja2 template\n which will collect and process the vars on the fly and create the resulting configs dynamically\n\n\n\n\nDefining the vars for app config\n\n\nfile: roles/frontend/defaults/main.yml\n\n\n---\n# defaults file for frontend\n  app:\n    version: 1.5\n    env: LOCALDEV\n\n  fav:\n    color: white\n    fruit: orange\n    car: chevy\n    laptop: toshiba\n\n  dbconn:\n    host: localhost\n    user: root\n    pass: changeme\n    db: devopsdemo\n\n\n\n\nCreate directory and template file.  You could either use the commands below or directly create it from the graphical editor.\n\n\ncd roles/frontend\nmkdir templates\ntouch templates/config.ini.j2\n\n\n\n\nfile: roles/frontend/templates/config.ini.j2  \n\n\n\n[database]\nhostname = {{ dbconn['host'] }}\nusername = {{ dbconn['user'] }}\npassword = {{ dbconn['pass'] }}\ndbname = {{ dbconn['db'] }}\n\n[environment]\nenvironment = {{ app['env'] }}\n\n[prefs]\ncolor  = {{ fav['color'] }}\nfruit  = {{ fav['fruit'] }}\ncar    = {{ fav['car'] }}\nlaptop = {{ fav['laptop'] }}\n\n\n\n\n\nAdding task to generate  the  config from jinja2 template\n\n\nfile: roles/frontend/tasks/main.yml ( append the following code to the file)\n\n\n- name: add application configs\n  template:\n    src: config.ini.j2\n    dest: /var/www/html/app/config.ini\n    owner: apache\n    group: apache\n    mode: 0644\n\n\n\n\nNow, run the playbook, reload the application page and validate. You should also check http://IPADDRESS:81/app/prefs.php to view if it prints the preferences you defined in the default vars.   \n\n\nansible-playbook  app.yml\n\n\n\n\nBeyond defaults - Playing with vars precedence\n\n\nLets define the variables from couple of other places, to learn about the Precedence rules. We will create,\n\n group_vars\n\n playbook vars\n\n\nSince we are going to define the variables using multi level hashes,  define the way hashes behave when defined from multiple places.\n\n\nUpdate chap7/ansible.cfg and add the following,\n\n\nhash_behaviour=merge\n\n\n\n\nLets create group_vars and create a group \nprod\n to define vars common to all prod hosts.\n\n\ncd chap7\nmkdir group_vars\ncd group_vars\ntouch prod.yml\n\n\n\n\nEdit \ngroup_vars/prod.yml\n file and add the following contents,\n\n\n---\n  fav:\n    color: yellow\n    fruit: guava\n\n\n\n\nLets also add vars to playbook. Edit app.yml and add vars as below,\n\n\n---\n  - name: Playbook to configure App Servers\n    hosts: app\n    become: true\n    vars:\n      fav:\n        fruit: mango\n    roles:\n    - apache\n\n\n\n\nExecute the playbook and check the output\n\n\nansible-playbook app.yml\n\n\n\n\nIf you view the content of the html file generated, you would notice the following,\n\n\nh3\n color     : yellow \n/h3\n\n\nh3\n fruit     : mango \n/h3\n\n\nh3\n car       : chevy \n/h3\n\n\nh3\n laptop    : toshiba \n/h3\n\n\n\n\n\n\n\n\n\n\n\nfav item\n\n\nrole defaults\n\n\ngroup_vars\n\n\nplaybook_vars\n\n\n\n\n\n\n\n\n\n\ncolor\n\n\nwhite\n\n\nyellow\n\n\n\n\n\n\n\n\nfruit\n\n\norange\n\n\nguava\n\n\nmango\n\n\n\n\n\n\ncar\n\n\nchevy\n\n\n\n\n\n\n\n\n\n\nlaptop\n\n\ntoshiba\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvalue of color comes from group_vars/all.yml\n\n\nvalue of fruit comes from playbook vars\n\n\nvalue of car and laptop comes from role defaults\n\n\n\n\nRegistered  Variables\n\n\nLets create a playbook to run a shell command, register the result and display the value of registered variable.\n\n\nCreate \nregister.yml\n in chap6 directory\n\n\n---\n  - name: register variable example\n    hosts: local\n    tasks:\n      - name: run a shell command and register result\n        shell: \n/sbin/ifconfig eth1\n\n        register: result\n\n      - name: print registered variable\n        debug: var=result\n\n\n\n\nExecute the playbook to display information about the registered variable.\n\n\nansible-playbook  register.yml\n\n\n\n\nAdding support for Ubuntu\n\n\nApache role that we have developed supports only RedHat based systems at the moment. To add support for ubuntu (app2), we must handle platform specific differences.\n\n\ne.g.\n\n\n\n\n\n\n\n\n\n\nRedHat\n\n\nDebian\n\n\n\n\n\n\n\n\n\n\nPackage Name\n\n\nhttpd\n\n\napache2\n\n\n\n\n\n\nService Name\n\n\nhttpd\n\n\napache2\n\n\n\n\n\n\n\n\nOS specific configurations can be defined by creating role vars and by including those in tasks.\n\n\nfile: roles/apache/vars/RedHat.yml\n\n\n---\napache:\n  package:\n    name: httpd\n  service:\n    name: httpd\n    status: started\n\n\n\n\nfile: roles/apache/vars/Debian.yml\n\n\n---\napache:\n  package:\n    name: apache2\n  service:\n    name: apache2\n    status: started\n\n\n\n\nLets now selectively include those var files from tasks/main.yml .  Also selectively call configurations.\nfile: role/apache/tasks/main.yml\n\n\n---\n# tasks file for apache\n  - include_vars: \n{{ ansible_os_family }}.yml\n\n  - include: install.yml\n  - include: start.yml\n  - include: config_{{ ansible_os_family }}.yml  \n\n\n\n\nWe are now going to create two different config tasks. Since the current config is applicable to RedHat, lets rename it to config_RedHat.yml\n\n\nmv roles/apache/tasks/config.yml roles/apache/tasks/config_RedHat.yml\n\n\n\n\nWe will now create a new config for Debian\n\n\nfile: roles/apache/tasks/config_Debian.yml\n\n\n- name: Copying index.html file...\n  template: \n\n    src=index.html.j2\n    dest=/var/www/html/index.html\n    mode=0777\n\n\n\n\nUpdate tasks and handlers to install and start the correct service\n\n\ntasks/install.yml\n\n\n---\n  - name: install httpd on centos\n    package: \n\n      name={{ apache['package']['name']}}\n      state=installed\n\n\n\n\ntasks/start.yml\n\n\n---\n  - name: start httpd service\n    service: \n\n      name={{ apache['service']['name']}}\n      state={{ apache['service']['status']}}\n\n\n\n\nhandlers/main.yml\n\n\n---\n# handlers file for apache\n  - name: restart apache service\n    service: \n\n      name={{ apache['service']['name']}}\n      state=restarted\n\n\n\n\nExercises\n\n\n\n\nCreate host specific variables in host_vars/HOSTNAME for one of the app servers, and define some variables values specific to the host. See the output after applying playbook on this node.\n\n\nGenerate MySQL Configurations dynamically using templates and modules.\n\n\nCreate a template for my.cnf.  Name it as roles/mysql/templates/my.cnf.j2\n\n\nReplace parameter values with templates variables\n\n\nDefine variables in role defaults.", 
            "title": "Making roles reusable with vars and templates"
        }, 
        {
            "location": "/templates_and_variables/#templates-and-variables", 
            "text": "In this  tutorial, we  are going to make the roles that we created earlier dynamically by adding templates and defining variables.", 
            "title": "Templates and Variables"
        }, 
        {
            "location": "/templates_and_variables/#variables", 
            "text": "Variables are of two types   Automatic Variables/ Facts  User Defined Variables   Lets try to discover information about our systems by using facts.", 
            "title": "Variables"
        }, 
        {
            "location": "/templates_and_variables/#finding-facts-about-systems", 
            "text": "Run the following command to see to facts of db servers     ansible db -m setup  [Output]  192.168.61.11 | SUCCESS =  {\n         ansible_facts : {\n             ansible_all_ipv4_addresses : [\n                 10.0.2.15 ,\n                 192.168.61.11 \n            ],\n             ansible_all_ipv6_addresses : [\n                 fe80::a00:27ff:fe30:3251 ,\n                 fe80::a00:27ff:fe8e:83e0 \n.....\n                 tz_offset :  +0100 ,\n                 weekday :  Monday ,\n                 weekday_number :  1 ,\n                 weeknumber :  36 ,\n                 year :  2016 \n            }", 
            "title": "Finding Facts About Systems"
        }, 
        {
            "location": "/templates_and_variables/#filtering-facts", 
            "text": "Use filter attribute to extract specific data   ansible db -m setup -a  filter=ansible_distribution   [Output]  192.168.61.11 | SUCCESS =  {\n   ansible_facts : {\n       ansible_distribution :  CentOS \n  },\n   changed : false\n}", 
            "title": "Filtering facts"
        }, 
        {
            "location": "/templates_and_variables/#defining-release-versions-with-vars", 
            "text": "Currently, while deploying the application, the versions of the artifacts as well as release directories are defined statically. This should change to vars so that the version can be defined from one place, and dynamically so.  Define the default vars  file: roles/frontend/defaults/main.yml  ---\n# defaults file for frontend\n  app:\n    version: 1.5  Update tasks to use the var defined above,  file: roles/frontend/tasks/main.yml  - name: Download and extract the release\n  unarchive:\n    src: https://github.com/devopsdemoapps/devops-demo-app/archive/{{ app.version }}.tar.gz\n    dest: /opt/app/release\n    owner: apache\n    group: apache\n    creates: /opt/app/release/devops-demo-app-{{ app.version }}\n    remote_src: yes\n\n- name: create a symlink\n  file:\n    src: /opt/app/release/devops-demo-app-{{ app.version }}\n    dest: /var/www/html/app\n    owner: apache\n    group: apache\n    state: link  Try This :\n  * Run playbook and check whether the above code works\n  * Change the version e.g. 1.4 and check if it has any effect", 
            "title": "Defining release versions with vars"
        }, 
        {
            "location": "/templates_and_variables/#creating-application-configurations-dynamically", 
            "text": "Application configs are defined with  config.ini  file.  The version of config.ini as shipped with the application is as follows,  [database]\nhostname = DBHOST\nusername = SQLUSER\npassword = SQLPASSWORD\ndbname = SQLDBNAME\n\n[environment]\nenvironment = ENVNAME\n\n[prefs]\ncolor  = white\nfruit  = apple\ncar    = fiat\nlaptop = dell  You should be able to customize these configs. In order to do that, you need to do split this into 2 things as follows,   vars  which define the actual properties and allow you to change it from different places  a  jinja2 template  which will collect and process the vars on the fly and create the resulting configs dynamically", 
            "title": "Creating  application configurations dynamically"
        }, 
        {
            "location": "/templates_and_variables/#defining-the-vars-for-app-config", 
            "text": "file: roles/frontend/defaults/main.yml  ---\n# defaults file for frontend\n  app:\n    version: 1.5\n    env: LOCALDEV\n\n  fav:\n    color: white\n    fruit: orange\n    car: chevy\n    laptop: toshiba\n\n  dbconn:\n    host: localhost\n    user: root\n    pass: changeme\n    db: devopsdemo  Create directory and template file.  You could either use the commands below or directly create it from the graphical editor.  cd roles/frontend\nmkdir templates\ntouch templates/config.ini.j2  file: roles/frontend/templates/config.ini.j2    \n[database]\nhostname = {{ dbconn['host'] }}\nusername = {{ dbconn['user'] }}\npassword = {{ dbconn['pass'] }}\ndbname = {{ dbconn['db'] }}\n\n[environment]\nenvironment = {{ app['env'] }}\n\n[prefs]\ncolor  = {{ fav['color'] }}\nfruit  = {{ fav['fruit'] }}\ncar    = {{ fav['car'] }}\nlaptop = {{ fav['laptop'] }}", 
            "title": "Defining the vars for app config"
        }, 
        {
            "location": "/templates_and_variables/#adding-task-to-generate-the-config-from-jinja2-template", 
            "text": "file: roles/frontend/tasks/main.yml ( append the following code to the file)  - name: add application configs\n  template:\n    src: config.ini.j2\n    dest: /var/www/html/app/config.ini\n    owner: apache\n    group: apache\n    mode: 0644  Now, run the playbook, reload the application page and validate. You should also check http://IPADDRESS:81/app/prefs.php to view if it prints the preferences you defined in the default vars.     ansible-playbook  app.yml", 
            "title": "Adding task to generate  the  config from jinja2 template"
        }, 
        {
            "location": "/templates_and_variables/#beyond-defaults-playing-with-vars-precedence", 
            "text": "Lets define the variables from couple of other places, to learn about the Precedence rules. We will create,  group_vars  playbook vars  Since we are going to define the variables using multi level hashes,  define the way hashes behave when defined from multiple places.  Update chap7/ansible.cfg and add the following,  hash_behaviour=merge  Lets create group_vars and create a group  prod  to define vars common to all prod hosts.  cd chap7\nmkdir group_vars\ncd group_vars\ntouch prod.yml  Edit  group_vars/prod.yml  file and add the following contents,  ---\n  fav:\n    color: yellow\n    fruit: guava  Lets also add vars to playbook. Edit app.yml and add vars as below,  ---\n  - name: Playbook to configure App Servers\n    hosts: app\n    become: true\n    vars:\n      fav:\n        fruit: mango\n    roles:\n    - apache  Execute the playbook and check the output  ansible-playbook app.yml  If you view the content of the html file generated, you would notice the following,  h3  color     : yellow  /h3  h3  fruit     : mango  /h3  h3  car       : chevy  /h3  h3  laptop    : toshiba  /h3      fav item  role defaults  group_vars  playbook_vars      color  white  yellow     fruit  orange  guava  mango    car  chevy      laptop  toshiba        value of color comes from group_vars/all.yml  value of fruit comes from playbook vars  value of car and laptop comes from role defaults", 
            "title": "Beyond defaults - Playing with vars precedence"
        }, 
        {
            "location": "/templates_and_variables/#registered-variables", 
            "text": "Lets create a playbook to run a shell command, register the result and display the value of registered variable.  Create  register.yml  in chap6 directory  ---\n  - name: register variable example\n    hosts: local\n    tasks:\n      - name: run a shell command and register result\n        shell:  /sbin/ifconfig eth1 \n        register: result\n\n      - name: print registered variable\n        debug: var=result  Execute the playbook to display information about the registered variable.  ansible-playbook  register.yml", 
            "title": "Registered  Variables"
        }, 
        {
            "location": "/templates_and_variables/#adding-support-for-ubuntu", 
            "text": "Apache role that we have developed supports only RedHat based systems at the moment. To add support for ubuntu (app2), we must handle platform specific differences.  e.g.      RedHat  Debian      Package Name  httpd  apache2    Service Name  httpd  apache2     OS specific configurations can be defined by creating role vars and by including those in tasks.  file: roles/apache/vars/RedHat.yml  ---\napache:\n  package:\n    name: httpd\n  service:\n    name: httpd\n    status: started  file: roles/apache/vars/Debian.yml  ---\napache:\n  package:\n    name: apache2\n  service:\n    name: apache2\n    status: started  Lets now selectively include those var files from tasks/main.yml .  Also selectively call configurations.\nfile: role/apache/tasks/main.yml  ---\n# tasks file for apache\n  - include_vars:  {{ ansible_os_family }}.yml \n  - include: install.yml\n  - include: start.yml\n  - include: config_{{ ansible_os_family }}.yml    We are now going to create two different config tasks. Since the current config is applicable to RedHat, lets rename it to config_RedHat.yml  mv roles/apache/tasks/config.yml roles/apache/tasks/config_RedHat.yml  We will now create a new config for Debian  file: roles/apache/tasks/config_Debian.yml  - name: Copying index.html file...\n  template:  \n    src=index.html.j2\n    dest=/var/www/html/index.html\n    mode=0777  Update tasks and handlers to install and start the correct service  tasks/install.yml  ---\n  - name: install httpd on centos\n    package:  \n      name={{ apache['package']['name']}}\n      state=installed  tasks/start.yml  ---\n  - name: start httpd service\n    service:  \n      name={{ apache['service']['name']}}\n      state={{ apache['service']['status']}}  handlers/main.yml  ---\n# handlers file for apache\n  - name: restart apache service\n    service:  \n      name={{ apache['service']['name']}}\n      state=restarted", 
            "title": "Adding support for Ubuntu"
        }, 
        {
            "location": "/templates_and_variables/#exercises", 
            "text": "Create host specific variables in host_vars/HOSTNAME for one of the app servers, and define some variables values specific to the host. See the output after applying playbook on this node.  Generate MySQL Configurations dynamically using templates and modules.  Create a template for my.cnf.  Name it as roles/mysql/templates/my.cnf.j2  Replace parameter values with templates variables  Define variables in role defaults.", 
            "title": "Exercises"
        }, 
        {
            "location": "/galaxy/", 
            "text": "Setting up loadbalancer with a galaxy role\n\n\nansible-galaxy install geerlingguy.haproxy\n\n\n\n\nlb.yml\n\n\n- hosts: lb\n  become: yes\n  roles:\n    - { role: geerlingguy.haproxy }\n\n\n\n\n\nhttp://IPADDRESS/app\n\n\ngroup_vars/prod.yml\n\n\nhaproxy_backend_httpchk: ''\nhaproxy_backend_servers:\n  - name: app1\n    address: 192.168.61.12:80\n  - name: app2\n    address: 192.168.61.13:80\n\n\n\n\n\nhttp://IPADDRESS/app", 
            "title": "Setting up Load Balancer with Ansible Galaxy"
        }, 
        {
            "location": "/galaxy/#setting-up-loadbalancer-with-a-galaxy-role", 
            "text": "ansible-galaxy install geerlingguy.haproxy  lb.yml  - hosts: lb\n  become: yes\n  roles:\n    - { role: geerlingguy.haproxy }  http://IPADDRESS/app  group_vars/prod.yml  haproxy_backend_httpchk: ''\nhaproxy_backend_servers:\n  - name: app1\n    address: 192.168.61.12:80\n  - name: app2\n    address: 192.168.61.13:80  http://IPADDRESS/app", 
            "title": "Setting up loadbalancer with a galaxy role"
        }, 
        {
            "location": "/control_structures/", 
            "text": "Control Structures\n\n\nIn Chapter 7, we will learn about the aspects of conditionals and iterations that affects program's execution flow in Ansible\n\nControl structures are of two different type\n\n\n\n\nConditional  \n\n\nIterative  \n\n\n\n\nUsing Iterators\n\n\nIteration over list of items\n\n\n\n\nCreate a list of packages  \n\n\nLet us create the following list of packages in base role.  \n\n\nEdit \ngroup_vars/prod.yml\n and put\n\n\n\n\n  systems:\n    packages:\n      - ntp\n      - tree\n      - vim\n\n\n\n\n\n\n\nAlso edit \nroles/systems/tasks/main.yml\n to iterate over this list of items and install packages\n\n\n\n\n- name: install common systems packages\n  package:  \n    name:  \n{{ item }}\n\n    state: installed\n  with_items:\n    - \n{{ systems.packages }}\n\n\n\n\n\napply\n\n\n  ansible-playbook app.yml\n\n\n\n\nIterating over a Hash Table/Dictionary\n\n\n\n\nThis iteration can be done with using \nwith_dict\n statement, let us see how.\n\n\nEdit \ngroup_vars/all\n file from the \nparent directory\n and define a dictionary of systems  users to be managed\n\n\n\n\nusers:\n   admin:\n     uid: 5001\n     shell: /bin/bash\n     home: /home/admin\n     state: present\n   dojo:\n     state: absent\n\n\n\n\n\n\nUpdate\n the systems task to iterate over this dictionary\n\n\n\n\nfile:  \nroles/systems/tasks/main.yml\n\n\n- name: create systems users\n  user:\n    name: \n{{ item.key }}\n\n    uid:  \n{{ item.value.uid  }}\n\n    shell: \n{{ item.value.shell  }}\n\n    home: \n{{ item.value.home    }}\n\n    state: \n{{ item.value.state   }}\n\n  with_dict: \n{{ users }}\n\n\n\n\n\n\n\nExecute the \napp\n playbook to verify the output\n\n\n\n\nansible-playbook app.yml\n\n\n\n\nTroubleshooting\n\n\nThe above playbook run fails as you have not defined all the fields for user \ndojo\n as part of the dictionary. You could either define it as part of vars, or better set defaults while invoking the vars, so that ansible automatically falls back to it.\n\n\nfile:  \nroles/systems/tasks/main.yml\n\n\n- name: create systems users\n   user:\n     name: \n{{ item.key }}\n\n     uid:  \n{{ item.value.uid | default('none') }}\n\n     shell: \n{{ item.value.shell | default('none') }}\n\n     home: \n{{ item.value.home  | default('none')  }}\n\n     state: \n{{ item.value.state  | default('none') }}\n\n   with_dict: \n{{ users }}\n\n\n\n\n\n\nApply and validate\n\n\nansible-playbook app.yml\n\n\n\n\nConditionals\n\n\nConditionals structures allow Ansible to choose an alternate path. Ansible does this by using \nwhen\n statements\n\n\nWhen\n statements\n\n\nWhen statement becomes helpful, when you will want to skip a particular step on a particular host\n\n\nSelectively calling install tasks based on platform\n\n\n\n\nEdit \nroles/apache/tasks/main.yml\n,\n\n\n\n\n---\n- include: install.yml\n  when: ansible_os_family == 'RedHat'\n- include: start.yml\n- include: config.yml\n\n\n\n\n\n\nThis will include \ninstall.yml\n only if the OS family is Redhat, otherwise it will skip the installation playbook\n\n\n\n\nConditional Execution of Roles\n\n\n\n\nThis will execute app playbook only if the node is running \nRedHat\n family\n\n\nUpdate app.yml to restrict role to be run only on RedHat platform.\n\n\n\n\n---\n  - name: Playbook to configure App Servers\n    hosts: app\n    become: true\n    vars:\n      fav:\n        fruit: mango\n    roles:\n    - apache\n    - php\n    - { role: frontend, when: ansible_os_family == 'RedHat' }\n\n\n\n\n\nAdding conditionals in Jinja2 templates\n\n\n\n\nPut the following content in \nroles/mysql/templates/my.cnf.j2\n\n\n\n\n[mysqld]\n\n{% if mysql.config.datadir is defined %}\ndatadir={{ mysql['config']['datadir'] }}\n{% endif %}\n\n{% if mysql.config.socket is defined %}\nsocket={{ mysql['config']['socket'] }}\n{% endif %}\n\nsymbolic-links=0\nlog-error=/var/log/mysqld.log\n\n{% if mysql.config.pid is defined %}\npid-file={{ mysql['config']['pid']}}\n{% endif %}\n\n[client]\nuser=root\npassword={{ mysql_root_db_pass }}\n\n\n\n\n\n\n\n\nThese conditions will run flawlessly, because we have already defined these Variables\n\n\n\n\n\n\nLet's run this code\n\n\n\n\n\n\nansible-playbook site.yml\n\n\n\n\n[Output]\n\n\nTASK [setup] *******************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : create admin user] ************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : remove dojo] ******************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : install tree] *****************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : install ntp] ******************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : start ntp service] ************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Installing Apache...] *******************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Starting Apache...] *********************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Creating configuration from templates...] ***********************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Copying index.html file...] *************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\n\n\n\n\nExercise\n: Try using \nDebian\n instead of \nRedHat\n . You shall see app role being skipped altogether. Don't forget to put it back after you try this out.\n\n\nExercises\n\n\n\n\nDefine dictionary of properties for a new database user  in group_vars/all. Observe if it gets created automatically  output by running db.yml playbook. Validate if the user is been actually present by logging on to the mysql server and checking status.\n\n\nUpdate index.html.j2 to iterate over the dictionary of favorites and generate html content to display it instead of adding multiple lines.\n\n\nDefine a hash/dictionary  of apache virtual hosts to be created  and create a template which would iterate over that dictionary and create vhost configurations.\n\n\nLearn about what else you could loop over, as well as how to do so by reading this document http://docs.ansible.com/ansible/playbooks_loops.html#id12", 
            "title": "Controlling execution flow with conditionals, iterators and tags"
        }, 
        {
            "location": "/control_structures/#control-structures", 
            "text": "In Chapter 7, we will learn about the aspects of conditionals and iterations that affects program's execution flow in Ansible \nControl structures are of two different type   Conditional    Iterative", 
            "title": "Control Structures"
        }, 
        {
            "location": "/control_structures/#using-iterators", 
            "text": "", 
            "title": "Using Iterators"
        }, 
        {
            "location": "/control_structures/#iteration-over-list-of-items", 
            "text": "Create a list of packages    Let us create the following list of packages in base role.    Edit  group_vars/prod.yml  and put     systems:\n    packages:\n      - ntp\n      - tree\n      - vim   Also edit  roles/systems/tasks/main.yml  to iterate over this list of items and install packages   - name: install common systems packages\n  package:  \n    name:   {{ item }} \n    state: installed\n  with_items:\n    -  {{ systems.packages }}   apply    ansible-playbook app.yml", 
            "title": "Iteration over list of items"
        }, 
        {
            "location": "/control_structures/#iterating-over-a-hash-tabledictionary", 
            "text": "This iteration can be done with using  with_dict  statement, let us see how.  Edit  group_vars/all  file from the  parent directory  and define a dictionary of systems  users to be managed   users:\n   admin:\n     uid: 5001\n     shell: /bin/bash\n     home: /home/admin\n     state: present\n   dojo:\n     state: absent   Update  the systems task to iterate over this dictionary   file:   roles/systems/tasks/main.yml  - name: create systems users\n  user:\n    name:  {{ item.key }} \n    uid:   {{ item.value.uid  }} \n    shell:  {{ item.value.shell  }} \n    home:  {{ item.value.home    }} \n    state:  {{ item.value.state   }} \n  with_dict:  {{ users }}    Execute the  app  playbook to verify the output   ansible-playbook app.yml", 
            "title": "Iterating over a Hash Table/Dictionary"
        }, 
        {
            "location": "/control_structures/#troubleshooting", 
            "text": "The above playbook run fails as you have not defined all the fields for user  dojo  as part of the dictionary. You could either define it as part of vars, or better set defaults while invoking the vars, so that ansible automatically falls back to it.  file:   roles/systems/tasks/main.yml  - name: create systems users\n   user:\n     name:  {{ item.key }} \n     uid:   {{ item.value.uid | default('none') }} \n     shell:  {{ item.value.shell | default('none') }} \n     home:  {{ item.value.home  | default('none')  }} \n     state:  {{ item.value.state  | default('none') }} \n   with_dict:  {{ users }}   Apply and validate  ansible-playbook app.yml", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/control_structures/#conditionals", 
            "text": "Conditionals structures allow Ansible to choose an alternate path. Ansible does this by using  when  statements", 
            "title": "Conditionals"
        }, 
        {
            "location": "/control_structures/#when-statements", 
            "text": "When statement becomes helpful, when you will want to skip a particular step on a particular host", 
            "title": "When statements"
        }, 
        {
            "location": "/control_structures/#selectively-calling-install-tasks-based-on-platform", 
            "text": "Edit  roles/apache/tasks/main.yml ,   ---\n- include: install.yml\n  when: ansible_os_family == 'RedHat'\n- include: start.yml\n- include: config.yml   This will include  install.yml  only if the OS family is Redhat, otherwise it will skip the installation playbook", 
            "title": "Selectively calling install tasks based on platform"
        }, 
        {
            "location": "/control_structures/#conditional-execution-of-roles", 
            "text": "This will execute app playbook only if the node is running  RedHat  family  Update app.yml to restrict role to be run only on RedHat platform.   ---\n  - name: Playbook to configure App Servers\n    hosts: app\n    become: true\n    vars:\n      fav:\n        fruit: mango\n    roles:\n    - apache\n    - php\n    - { role: frontend, when: ansible_os_family == 'RedHat' }", 
            "title": "Conditional Execution of Roles"
        }, 
        {
            "location": "/control_structures/#adding-conditionals-in-jinja2-templates", 
            "text": "Put the following content in  roles/mysql/templates/my.cnf.j2   [mysqld]\n\n{% if mysql.config.datadir is defined %}\ndatadir={{ mysql['config']['datadir'] }}\n{% endif %}\n\n{% if mysql.config.socket is defined %}\nsocket={{ mysql['config']['socket'] }}\n{% endif %}\n\nsymbolic-links=0\nlog-error=/var/log/mysqld.log\n\n{% if mysql.config.pid is defined %}\npid-file={{ mysql['config']['pid']}}\n{% endif %}\n\n[client]\nuser=root\npassword={{ mysql_root_db_pass }}    These conditions will run flawlessly, because we have already defined these Variables    Let's run this code    ansible-playbook site.yml  [Output]  TASK [setup] *******************************************************************\nok: [192.168.61.12]\nok: [192.168.61.13]\n\nTASK [base : create admin user] ************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : remove dojo] ******************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : install tree] *****************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : install ntp] ******************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [base : start ntp service] ************************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Installing Apache...] *******************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Starting Apache...] *********************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Creating configuration from templates...] ***********************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]\n\nTASK [apache : Copying index.html file...] *************************************\nskipping: [192.168.61.12]\nskipping: [192.168.61.13]  Exercise : Try using  Debian  instead of  RedHat  . You shall see app role being skipped altogether. Don't forget to put it back after you try this out.", 
            "title": "Adding conditionals in Jinja2 templates"
        }, 
        {
            "location": "/control_structures/#exercises", 
            "text": "Define dictionary of properties for a new database user  in group_vars/all. Observe if it gets created automatically  output by running db.yml playbook. Validate if the user is been actually present by logging on to the mysql server and checking status.  Update index.html.j2 to iterate over the dictionary of favorites and generate html content to display it instead of adding multiple lines.  Define a hash/dictionary  of apache virtual hosts to be created  and create a template which would iterate over that dictionary and create vhost configurations.  Learn about what else you could loop over, as well as how to do so by reading this document http://docs.ansible.com/ansible/playbooks_loops.html#id12", 
            "title": "Exercises"
        }, 
        {
            "location": "/magicvars_envs/", 
            "text": "placeholder", 
            "title": "Auto discovery and multi environments"
        }, 
        {
            "location": "/magicvars_envs/#placeholder", 
            "text": "", 
            "title": "placeholder"
        }, 
        {
            "location": "/ansible-vault/", 
            "text": "Why to use Vault\n\n\n\n\nTo maintain sensitive data e.g. passwords/creds, keys etc.\n\n\nVersion control encrypted files instead of plain text\n\n\nansible-vault utility\n\n\n\n\nHowe ?\n\n\n\n\nUsed AES Cipher\n\n\nSymmetric Key\n\n\n\n\nWhat can be encrypted ?\n\n\n\n\nStructured data (yaml, json)\n\n\nVar files\n\n\ngroup_vars/hostvars\n\n\ninclude_vars or  var_files\n\n\nvar files passed at command line with \"-e @file\"\n\n\n\n\n\n\nTasks (however not very common)\n\n\nArbitory Files\n\n\n\n\nWhat can not be encrypted ?\n\n\n\n\nTemplates\n\n\n\n\nHow to encrypt/decrypt\n\n\n\n\nUsing --ask-vault-pass\n\n\nUsing --vault-password-file\n\n\n\n\nansible-vault Operations\n\n\n\n\nencrypt\n\n\ndecrypt\n\n\ncreate\n\n\nrekey\n\n\nedit\n\n\n\n\nRunning Playbooks with Vault\n\n\nansible-playbook site.yml --ask-vault-pass\nansible-playbook site.yml --vault-password-file ~/.vault_pass.txt\n\n\n\n\nAutomating Rekeying Process\n\n\n--new-vault-password-file=NEW_VAULT_PASSWORD_FILE\n\n\n\n\n\n                   new vault password file for rekey\n\n\n\nLab:\n\n\nansible-vault encrypt roles/mysql/defaults/main.yml\nansible-vault encrypt group_vars/all.yml\nansible-vault view group_vars/all.yml\nansible-playbook site.yml --ask-vault-pass", 
            "title": "Encrypting sensitive data with Ansible Vault"
        }, 
        {
            "location": "/ansible-vault/#why-to-use-vault", 
            "text": "To maintain sensitive data e.g. passwords/creds, keys etc.  Version control encrypted files instead of plain text  ansible-vault utility", 
            "title": "Why to use Vault"
        }, 
        {
            "location": "/ansible-vault/#howe", 
            "text": "Used AES Cipher  Symmetric Key", 
            "title": "Howe ?"
        }, 
        {
            "location": "/ansible-vault/#what-can-be-encrypted", 
            "text": "Structured data (yaml, json)  Var files  group_vars/hostvars  include_vars or  var_files  var files passed at command line with \"-e @file\"    Tasks (however not very common)  Arbitory Files", 
            "title": "What can be encrypted ?"
        }, 
        {
            "location": "/ansible-vault/#what-can-not-be-encrypted", 
            "text": "Templates", 
            "title": "What can not be encrypted ?"
        }, 
        {
            "location": "/ansible-vault/#how-to-encryptdecrypt", 
            "text": "Using --ask-vault-pass  Using --vault-password-file", 
            "title": "How to encrypt/decrypt"
        }, 
        {
            "location": "/ansible-vault/#ansible-vault-operations", 
            "text": "encrypt  decrypt  create  rekey  edit", 
            "title": "ansible-vault Operations"
        }, 
        {
            "location": "/ansible-vault/#running-playbooks-with-vault", 
            "text": "ansible-playbook site.yml --ask-vault-pass\nansible-playbook site.yml --vault-password-file ~/.vault_pass.txt", 
            "title": "Running Playbooks with Vault"
        }, 
        {
            "location": "/ansible-vault/#automating-rekeying-process", 
            "text": "--new-vault-password-file=NEW_VAULT_PASSWORD_FILE                     new vault password file for rekey  Lab:  ansible-vault encrypt roles/mysql/defaults/main.yml\nansible-vault encrypt group_vars/all.yml\nansible-vault view group_vars/all.yml\nansible-playbook site.yml --ask-vault-pass", 
            "title": "Automating Rekeying Process"
        }, 
        {
            "location": "/update/", 
            "text": "Updating Application\n\n\nWriting database schema update playbook\n\n\nThe following url has the links to the database dumps\nhttps://github.com/devopsdemoapps/devops-demo-app/tree/master/data\n\n\nLets write tasks to update schema for database\n\n\n---\n  - name: playbook to configure db servers\n    hosts: db\n    become: yes\n    roles:\n      - { role: geerlingguy.mysql }\n    tasks:\n      - name: download database schema\n        get_url:\n          url: https://raw.githubusercontent.com/devopsdemoapps/devops-demo-app/master/data/devops-demo-1.0.sql\n          dest: /tmp/devops-demo-1.0.sql\n          mode: 0444\n        tags: schema\n\n      - name: Schema Migrate\n        mysql_db:\n          name: \n{{ dbconn.db }}\n\n          login_host: \n127.0.0.1\n\n          login_password: \n{{ mysql_root_password }}\n\n          login_user: \nroot\n\n          state: import\n          target: /tmp/devops-demo-1.0.sql\n        tags: schema\n\n\n\n\n\nwhere,\n  * we have added task to download the schema file from a remote uri\n  * load the schema using \nimport\n state\n\n\nIts important to use \n1.0\n as the schema version first time you run as it needs the initial tables.\n\n\nRun it for the first time,\n\n\nansible-playbook db.yml  --vault-id prod@~/.vault_prod --tags=schema\n\n\n\n\nOnce the initial schema apply is done, update the tasks to use \napp.version\n for schema update as well\n\n\n---\n  - name: playbook to configure db servers\n    hosts: db\n    become: yes\n    roles:\n      - { role: geerlingguy.mysql }\n    tasks:\n      - name: download database schema\n        get_url:\n          url: https://raw.githubusercontent.com/devopsdemoapps/devops-demo-app/master/data/devops-demo-{{ app.version }}.sql\n          dest: /tmp/devops-demo-{{ app.version }}.sql\n          mode: 0444\n        tags: schema\n\n      - name: Schema Migrate\n        mysql_db:\n          name: \n{{ dbconn.db }}\n\n          login_host: \n127.0.0.1\n\n          login_password: \n{{ mysql_root_password }}\n\n          login_user: \nroot\n\n          state: import\n          target: /tmp/devops-demo-{{ app.version }}.sql\n        tags: schema\n\n\n\n\n\nDeployment Playbook\n\n\nStrategy:\n  * Rolling updates/zero downtime\n  * Batch size = 1\n\n\nDeployment strategy and sequence,\n  * Update database schema\n  * For each web server update\n    * Disable load balancer traffic\n    * Deploy new version of the application\n    * Wait for the app to be available\n    * Enable traffic from load balancer\n\n\nPre tasks\n\n\nConfigure haproxy socket\n\n\nfile: group_vars/prod.yml\n\n\n\n  haproxy_socket: /var/run/haproxy.sock\n\n\n\n\n\nInstall netcat on lb\n\n\nansible lb -b -m yum -a \nname=nc state=installed\n\n\n\n\n\nCreate update script\n\n\nfile: update.yml\n\n\n---\n# Playbook for updating web server in batches\n# filename: update.yml\n\n- hosts: app\n  become: yes\n  vars:\n    app:\n      version: 1.6\n  serial: 1\n\n  pre_tasks:\n    - name: take the app server out\n      haproxy:\n        host: '{{ inventory_hostname }}'\n        state: disabled\n      delegate_to: lb\n\n  roles:\n     - frontend\n\n  post_tasks:\n    - name: add server to loadbalancer\n      haproxy:\n        host: '{{ inventory_hostname }}'\n        state: enabled\n      delegate_to: lb\n\n\n\n\n\nDeploy a new version with\n\n\nansible-playbook update.yml", 
            "title": "Setting up Zero Downtime Deployment"
        }, 
        {
            "location": "/update/#updating-application", 
            "text": "", 
            "title": "Updating Application"
        }, 
        {
            "location": "/update/#writing-database-schema-update-playbook", 
            "text": "The following url has the links to the database dumps\nhttps://github.com/devopsdemoapps/devops-demo-app/tree/master/data  Lets write tasks to update schema for database  ---\n  - name: playbook to configure db servers\n    hosts: db\n    become: yes\n    roles:\n      - { role: geerlingguy.mysql }\n    tasks:\n      - name: download database schema\n        get_url:\n          url: https://raw.githubusercontent.com/devopsdemoapps/devops-demo-app/master/data/devops-demo-1.0.sql\n          dest: /tmp/devops-demo-1.0.sql\n          mode: 0444\n        tags: schema\n\n      - name: Schema Migrate\n        mysql_db:\n          name:  {{ dbconn.db }} \n          login_host:  127.0.0.1 \n          login_password:  {{ mysql_root_password }} \n          login_user:  root \n          state: import\n          target: /tmp/devops-demo-1.0.sql\n        tags: schema  where,\n  * we have added task to download the schema file from a remote uri\n  * load the schema using  import  state  Its important to use  1.0  as the schema version first time you run as it needs the initial tables.  Run it for the first time,  ansible-playbook db.yml  --vault-id prod@~/.vault_prod --tags=schema  Once the initial schema apply is done, update the tasks to use  app.version  for schema update as well  ---\n  - name: playbook to configure db servers\n    hosts: db\n    become: yes\n    roles:\n      - { role: geerlingguy.mysql }\n    tasks:\n      - name: download database schema\n        get_url:\n          url: https://raw.githubusercontent.com/devopsdemoapps/devops-demo-app/master/data/devops-demo-{{ app.version }}.sql\n          dest: /tmp/devops-demo-{{ app.version }}.sql\n          mode: 0444\n        tags: schema\n\n      - name: Schema Migrate\n        mysql_db:\n          name:  {{ dbconn.db }} \n          login_host:  127.0.0.1 \n          login_password:  {{ mysql_root_password }} \n          login_user:  root \n          state: import\n          target: /tmp/devops-demo-{{ app.version }}.sql\n        tags: schema", 
            "title": "Writing database schema update playbook"
        }, 
        {
            "location": "/update/#deployment-playbook", 
            "text": "Strategy:\n  * Rolling updates/zero downtime\n  * Batch size = 1  Deployment strategy and sequence,\n  * Update database schema\n  * For each web server update\n    * Disable load balancer traffic\n    * Deploy new version of the application\n    * Wait for the app to be available\n    * Enable traffic from load balancer", 
            "title": "Deployment Playbook"
        }, 
        {
            "location": "/update/#pre-tasks", 
            "text": "Configure haproxy socket  file: group_vars/prod.yml  \n  haproxy_socket: /var/run/haproxy.sock  Install netcat on lb  ansible lb -b -m yum -a  name=nc state=installed   Create update script  file: update.yml  ---\n# Playbook for updating web server in batches\n# filename: update.yml\n\n- hosts: app\n  become: yes\n  vars:\n    app:\n      version: 1.6\n  serial: 1\n\n  pre_tasks:\n    - name: take the app server out\n      haproxy:\n        host: '{{ inventory_hostname }}'\n        state: disabled\n      delegate_to: lb\n\n  roles:\n     - frontend\n\n  post_tasks:\n    - name: add server to loadbalancer\n      haproxy:\n        host: '{{ inventory_hostname }}'\n        state: enabled\n      delegate_to: lb  Deploy a new version with  ansible-playbook update.yml", 
            "title": "Pre tasks"
        }, 
        {
            "location": "/dynamic_inventory/", 
            "text": "Using dynamic inventory with ec2\n\n\nDownload the script to setup dynamic inventory\n\n\nwget -c  https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/ec2.py\n\n\n\n\ncreate \nec2.ini\n with crdentials to connect to aws.\n\n\nBest practice\n: its recommended you create a read only user and use the iam keys for the same with ansible. For dynamic inventory, Ansible does need any additional access to make changes. Its a recommended security practice.\n\n\nsample ec2.ini\n\n\n[profile staging]\nexport AWS_ACCESS_KEY_ID='AKJKHKSDHFSJHJD73NEQ2Q'\nexport AWS_SECRET_ACCESS_KEY='aUy56Ksmw2bD/Aepmsdge3KsasnMSJIHls209NZpTc7'\n\n\n\n\nits also recommeded you store this file somewhere securely with least privileges\n\n\ne.g.\n\n\nmv ec2.ini ~/.ec2.ini\nchmod 400 ~/.ec2.ini\n\n\n\n\n\nSet the path to ini file\n\n\nexport EC2_INI_PATH=~/.ec2.ini\n\n\nCreate a local ansible configuration\n\n\n[defaults]\nremote_user = ubuntu\ninventory   = ec2.py\nretry_files_save_path = /tmp\nhost_key_checking = False\nlog_path=ansible.log\n\n\n\n\nNow test the dynamic inventory script\n\n\nexamples (update as per your profile and instance configs)\n\n\n./ec2.py --list\n\n./ec2.py --profile demo\n\n/ec2.py --host 38.105.83.147\n\n\n\n\nThis should connect to aws, fetch information and display groups dynamically fetched.\n\n\nYou should now be ready to connect to the ec2 servers\n\n\ne.g.\n\n\nansible all --list-hosts\nansible ec2 --list-hosts\nansible ec2 -m ping\nansible tag_env_demo -m ping\n\n\n\n\nWriting your own dynmaic inventory\n\n\nReferences:\n\n\nhttp://docs.ansible.com/ansible/latest/intro_dynamic_inventory.html \n\n\nhttp://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html\n\n\nhttps://www.jeffgeerling.com/blog/creating-custom-dynamic-inventories-ansible\n--list\n--host", 
            "title": "Dynamic Inventory"
        }, 
        {
            "location": "/dynamic_inventory/#using-dynamic-inventory-with-ec2", 
            "text": "Download the script to setup dynamic inventory  wget -c  https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/ec2.py  create  ec2.ini  with crdentials to connect to aws.  Best practice : its recommended you create a read only user and use the iam keys for the same with ansible. For dynamic inventory, Ansible does need any additional access to make changes. Its a recommended security practice.  sample ec2.ini  [profile staging]\nexport AWS_ACCESS_KEY_ID='AKJKHKSDHFSJHJD73NEQ2Q'\nexport AWS_SECRET_ACCESS_KEY='aUy56Ksmw2bD/Aepmsdge3KsasnMSJIHls209NZpTc7'  its also recommeded you store this file somewhere securely with least privileges  e.g.  mv ec2.ini ~/.ec2.ini\nchmod 400 ~/.ec2.ini  Set the path to ini file  export EC2_INI_PATH=~/.ec2.ini  Create a local ansible configuration  [defaults]\nremote_user = ubuntu\ninventory   = ec2.py\nretry_files_save_path = /tmp\nhost_key_checking = False\nlog_path=ansible.log  Now test the dynamic inventory script  examples (update as per your profile and instance configs)  ./ec2.py --list\n\n./ec2.py --profile demo\n\n/ec2.py --host 38.105.83.147  This should connect to aws, fetch information and display groups dynamically fetched.  You should now be ready to connect to the ec2 servers  e.g.  ansible all --list-hosts\nansible ec2 --list-hosts\nansible ec2 -m ping\nansible tag_env_demo -m ping", 
            "title": "Using dynamic inventory with ec2"
        }, 
        {
            "location": "/dynamic_inventory/#writing-your-own-dynmaic-inventory", 
            "text": "References:  http://docs.ansible.com/ansible/latest/intro_dynamic_inventory.html   http://docs.ansible.com/ansible/latest/dev_guide/developing_inventory.html  https://www.jeffgeerling.com/blog/creating-custom-dynamic-inventories-ansible\n--list\n--host", 
            "title": "Writing your own dynmaic inventory"
        }, 
        {
            "location": "/ansible-windows/", 
            "text": "Preparing Ansible host\n\n\nsudo yum install python-pip\nsudo pip install \npywinrm\n=0.1.1\n\n\n\n\n\nPreparing Windows Host\n\n\n\n\nCreate a file and paste the below github content and save \u201cPowerShell Scripts (* .ps1)\u201d.\n\n\n\n\nhttps://github.com/ansible/ansible/blob/devel/examples/scripts/ConfigureRemotingForAnsible.ps1\n\n\n\n\nDouble Click the file or open powershell and Execute it from the file path. winrm will be configured\n\n\n\n\nSetting up inventory and inventory vars\n\n\n\n\nAdd windows host to inventory by editing myhosts.ini\n\n\n\n\n    [windows]\n  windows_host_ip_or_hostname\n\n\n\n\nCreate group vars for windows group\n    - Create group_vars/windows.yml\n\n\n    ansible_ssh_user: \nadmin user\n\n    ansible_ssh_pass: \nadmin user password\n\n    ansible_ssh_port: 5986\n    ansible_connection: winrm\n    ansible_winrm_server_cert_validation: ignore\n\n\n\n\nValidate Connectivity\n\n\nansible windows -i host -m win_ping\nansible windows -i host -m setup\n\n\n\n\n\nCreate a Sample Playbook : windows.yml\n\n\n---\n- name: test raw module\n  hosts: windows\n  tasks:\n    - name: run ipconfig\n      raw: ipconfig\n      register: ipconfig\n\n    - debug: var=ipconfig\n\n    - name: test stat module on file\n      win_stat: path=\nC:/Windows/win.ini\n\n      register: stat_file\n\n    - debug: var=stat_file\n\n    - name: check stat_file result\n      assert:\n          that:\n             - \nstat_file.stat.exists\n\n             - \nnot stat_file.stat.isdir\n\n             - \nstat_file.stat.size \n 0\n\n             - \nstat_file.stat.md5\n\n\n    - name: Install IIS\n      win_feature:\n        name: \nWeb-Server\n\n        state: absent\n        restart: yes\n        include_sub_features: yes\n        include_management_tools: yes\n\n\n\n\nExecute Playbook as\n\n\nansible-playbook windows.yml\n\n\n\n\n\nReference\n\n\nhttp://darrylcauldwell.com/how-to-setup-an-ansible-test-lab-for-windows-managed-nodes-custom-windows-modules/\nhttp://docs.ansible.com/ansible/intro_windows.html", 
            "title": "Ansible on Windows"
        }, 
        {
            "location": "/ansible-windows/#preparing-ansible-host", 
            "text": "sudo yum install python-pip\nsudo pip install  pywinrm =0.1.1", 
            "title": "Preparing Ansible host"
        }, 
        {
            "location": "/ansible-windows/#preparing-windows-host", 
            "text": "Create a file and paste the below github content and save \u201cPowerShell Scripts (* .ps1)\u201d.   https://github.com/ansible/ansible/blob/devel/examples/scripts/ConfigureRemotingForAnsible.ps1   Double Click the file or open powershell and Execute it from the file path. winrm will be configured", 
            "title": "Preparing Windows Host"
        }, 
        {
            "location": "/ansible-windows/#setting-up-inventory-and-inventory-vars", 
            "text": "Add windows host to inventory by editing myhosts.ini       [windows]\n  windows_host_ip_or_hostname  Create group vars for windows group\n    - Create group_vars/windows.yml      ansible_ssh_user:  admin user \n    ansible_ssh_pass:  admin user password \n    ansible_ssh_port: 5986\n    ansible_connection: winrm\n    ansible_winrm_server_cert_validation: ignore  Validate Connectivity  ansible windows -i host -m win_ping\nansible windows -i host -m setup  Create a Sample Playbook : windows.yml  ---\n- name: test raw module\n  hosts: windows\n  tasks:\n    - name: run ipconfig\n      raw: ipconfig\n      register: ipconfig\n\n    - debug: var=ipconfig\n\n    - name: test stat module on file\n      win_stat: path= C:/Windows/win.ini \n      register: stat_file\n\n    - debug: var=stat_file\n\n    - name: check stat_file result\n      assert:\n          that:\n             -  stat_file.stat.exists \n             -  not stat_file.stat.isdir \n             -  stat_file.stat.size   0 \n             -  stat_file.stat.md5 \n\n    - name: Install IIS\n      win_feature:\n        name:  Web-Server \n        state: absent\n        restart: yes\n        include_sub_features: yes\n        include_management_tools: yes  Execute Playbook as  ansible-playbook windows.yml", 
            "title": "Setting up inventory and inventory vars"
        }, 
        {
            "location": "/ansible-windows/#reference", 
            "text": "http://darrylcauldwell.com/how-to-setup-an-ansible-test-lab-for-windows-managed-nodes-custom-windows-modules/\nhttp://docs.ansible.com/ansible/intro_windows.html", 
            "title": "Reference"
        }, 
        {
            "location": "/ansible-pull/", 
            "text": "ansible-pull -U  https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\n\n\n\n\nConnects to git and checks out the repo\n\n\nFinds fqdn.yml or local.yml  \n\n\nLaunches the playbook run\n\n\n\n\nansible-pull can\n  *   -C CHECKOUT, --checkout=CHECKOUT   Accept the git branch/tag/commit to Pull\n  *   -o, --only-if-changed\n\n\n  ansible-pull -U  https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nStarting Ansible Pull at 2016-11-15 14:54:53\n/usr/bin/ansible-pull -U https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nlocalhost | SUCCESS =\n {\n    \nafter\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nbefore\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nchanged\n: false,\n    \nwarnings\n: [\n        \nYour git version is too old to fully support the depth argument. Falling back to full checkouts.\n\n    ]\n}\n [WARNING]: Your git version is too old to fully support the depth argument. Falling\nback to full checkouts.\n\nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [create admin user] *******************************************************\nok: [localhost]\n\nTASK [remove dojo] *************************************************************\nok: [localhost]\n\nTASK [install tree] ************************************************************\nok: [localhost]\n\nTASK [install ntp] *************************************************************\nok: [localhost]\n\nTASK [start ntp service] *******************************************************\nok: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=6    changed=0    unreachable=0    failed=0\nansible-pull -U  https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nStarting Ansible Pull at 2016-11-15 14:54:53\n/usr/bin/ansible-pull -U https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nlocalhost | SUCCESS =\n {\n    \nafter\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nbefore\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nchanged\n: false,\n    \nwarnings\n: [\n        \nYour git version is too old to fully support the depth argument. Falling back to full checkouts.\n\n    ]\n}\n [WARNING]: Your git version is too old to fully support the depth argument. Falling\nback to full checkouts.\n\nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [create admin user] *******************************************************\nok: [localhost]\n\nTASK [remove dojo] *************************************************************\nok: [localhost]\n\nTASK [install tree] ************************************************************\nok: [localhost]\n\nTASK [install ntp] *************************************************************\nok: [localhost]\n\nTASK [start ntp service] *******************************************************\nok: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=6    changed=0    unreachable=0    failed=0", 
            "title": "Ansible Pull"
        }, 
        {
            "location": "/ansible-pull/", 
            "text": "ansible-pull -U  https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\n\n\n\n\nConnects to git and checks out the repo\n\n\nFinds fqdn.yml or local.yml  \n\n\nLaunches the playbook run\n\n\n\n\nansible-pull can\n  *   -C CHECKOUT, --checkout=CHECKOUT   Accept the git branch/tag/commit to Pull\n  *   -o, --only-if-changed\n\n\n  ansible-pull -U  https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nStarting Ansible Pull at 2016-11-15 14:54:53\n/usr/bin/ansible-pull -U https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nlocalhost | SUCCESS =\n {\n    \nafter\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nbefore\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nchanged\n: false,\n    \nwarnings\n: [\n        \nYour git version is too old to fully support the depth argument. Falling back to full checkouts.\n\n    ]\n}\n [WARNING]: Your git version is too old to fully support the depth argument. Falling\nback to full checkouts.\n\nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [create admin user] *******************************************************\nok: [localhost]\n\nTASK [remove dojo] *************************************************************\nok: [localhost]\n\nTASK [install tree] ************************************************************\nok: [localhost]\n\nTASK [install ntp] *************************************************************\nok: [localhost]\n\nTASK [start ntp service] *******************************************************\nok: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=6    changed=0    unreachable=0    failed=0\nansible-pull -U  https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nStarting Ansible Pull at 2016-11-15 14:54:53\n/usr/bin/ansible-pull -U https://github.com/schoolofdevops/ansible-repo -i myhosts.ini\nlocalhost | SUCCESS =\n {\n    \nafter\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nbefore\n: \nf93d954e4bdc3aebbe4fba690ceccf1f8285e508\n,\n    \nchanged\n: false,\n    \nwarnings\n: [\n        \nYour git version is too old to fully support the depth argument. Falling back to full checkouts.\n\n    ]\n}\n [WARNING]: Your git version is too old to fully support the depth argument. Falling\nback to full checkouts.\n\nPLAY [Base Configurations for ALL hosts] ***************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [create admin user] *******************************************************\nok: [localhost]\n\nTASK [remove dojo] *************************************************************\nok: [localhost]\n\nTASK [install tree] ************************************************************\nok: [localhost]\n\nTASK [install ntp] *************************************************************\nok: [localhost]\n\nTASK [start ntp service] *******************************************************\nok: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=6    changed=0    unreachable=0    failed=0", 
            "title": "Ansible Tower"
        }, 
        {
            "location": "/registered_variables/", 
            "text": "(src: http://docs.ansible.com/ansible/test_strategies.html)\n\n\ntasks:\n\n\n\n\n\n\naction: uri url=http://www.example.com return_content=yes\n    register: webpage\n\n\n\n\n\n\nfail: msg='service is not happy'\n    when: \"'AWESOME' not in webpage.content\"\n\n\n\n\n\n\ntasks:\n\n\n\n\n\n\nshell: /usr/bin/some-command --parameter value\n     register: cmd_result\n\n\n\n\n\n\nassert:\n       that:\n         - \"'not ready' not in cmd_result.stderr\"\n         - \"'gizmo enabled' in cmd_result.stdout\"\n\n\n\n\n\n\ntasks:\n\n\n\n\n\n\nstat: path=/path/to/something\n     register: p\n\n\n\n\n\n\nassert:\n       that:\n         - p.stat.exists and p.stat.isdir", 
            "title": "Registered Variable"
        }, 
        {
            "location": "/custom_modules/", 
            "text": "Creating a custiom Modules\n\n\nWriting module with bash\n\n\nmkdir library\ntouch library/mymodule\n\n\n\n\nfile: library/mymodule\n\n\n#!/bin/bash\n\ndisplay=\nThis is a simple bash module..\n\n\necho -e \n{\\\nmessage\\\n:\\\n$display\n\\\n}\n\n\n\n\n\n\nfile: custom_modules.yml\n\n\n---\n- hosts: local\n  sudo: yes\n  tasks:\n   - name: test custom module\n     mymodule:\n     register: uptime\n\n   - debug: var=uptime\n\n\n\n\nTest\n\n\nansible-playbook custom_module.yml\n\n\n\n\nAccepting module options\n\n\nfile: custom_modules.yml\n\n\n- name: check version\n  printversion:\n    app: java\n    appv: 3.4\n  register: printversion\n\n- debug: var=printversion\n\n\n\n\nfile: library/printversion\n\n\n#!/bin/bash\n#\n# This script accepts two inputs\n# 1. app\n# 2. appv\n# and prints it as a message\n\nchanged=\nfalse\n\n\nsource $1\n\ndisplay=\nReceived app  $app with version as $appv\n\n\n\n\nif [ \n$app\n == \npython\n ]; then\n  changed=\ntrue\n\nfi\n\nprintf '{\nchanged\n: %s, \nmsg\n: \n%s\n}' \n$changed\n \n$display\n\n\nexit 0\n\n\n\n\n\nTest\n\n\nansible-playbook custom_module.yml\n\n\n\n\nTrying out sample python module\n\n\ncd library\nwget -c https://gist.githubusercontent.com/initcron/88049b4fc3cbf4c53d17405efdd3a720/raw/fd2a4bccbe8fa895e3f6a6b517ec74abd1844df5/my_new_test_module\n\n\n\n\nfile: custom_modules.yml\n\n\n- name: run the new module\n  my_new_test_module:\n    name: 'hello'\n    new: true\n  register: testout\n\n- name: dump test output\n  debug:\n    msg: '{{ testout }}'\n\n\n\n\n\nTest\n\n\nansible-playbook custom_module.yml", 
            "title": "Custom Module"
        }, 
        {
            "location": "/custom_modules/#creating-a-custiom-modules", 
            "text": "", 
            "title": "Creating a custiom Modules"
        }, 
        {
            "location": "/custom_modules/#writing-module-with-bash", 
            "text": "mkdir library\ntouch library/mymodule  file: library/mymodule  #!/bin/bash\n\ndisplay= This is a simple bash module.. \n\necho -e  {\\ message\\ :\\ $display \\ }   file: custom_modules.yml  ---\n- hosts: local\n  sudo: yes\n  tasks:\n   - name: test custom module\n     mymodule:\n     register: uptime\n\n   - debug: var=uptime  Test  ansible-playbook custom_module.yml", 
            "title": "Writing module with bash"
        }, 
        {
            "location": "/custom_modules/#accepting-module-options", 
            "text": "file: custom_modules.yml  - name: check version\n  printversion:\n    app: java\n    appv: 3.4\n  register: printversion\n\n- debug: var=printversion  file: library/printversion  #!/bin/bash\n#\n# This script accepts two inputs\n# 1. app\n# 2. appv\n# and prints it as a message\n\nchanged= false \n\nsource $1\n\ndisplay= Received app  $app with version as $appv \n\n\n\nif [  $app  ==  python  ]; then\n  changed= true \nfi\n\nprintf '{ changed : %s,  msg :  %s }'  $changed   $display \n\nexit 0  Test  ansible-playbook custom_module.yml", 
            "title": "Accepting module options"
        }, 
        {
            "location": "/custom_modules/#trying-out-sample-python-module", 
            "text": "cd library\nwget -c https://gist.githubusercontent.com/initcron/88049b4fc3cbf4c53d17405efdd3a720/raw/fd2a4bccbe8fa895e3f6a6b517ec74abd1844df5/my_new_test_module  file: custom_modules.yml  - name: run the new module\n  my_new_test_module:\n    name: 'hello'\n    new: true\n  register: testout\n\n- name: dump test output\n  debug:\n    msg: '{{ testout }}'  Test  ansible-playbook custom_module.yml", 
            "title": "Trying out sample python module"
        }, 
        {
            "location": "/troubleshooting/", 
            "text": "Troubleshooting Techniques\n  * Using verbose mode\n  * Using --start-at-task\n  * Using --step\n  * Using debugger\n\n\nVerbose Mode\n -vvvv\n\n\nDebugger\n\n\nhttp://docs.ansible.com/ansible/playbooks_debugger.html\n\n\nDefining Failure\nfailed_when: \"'FAILED' in command_result.stderr\"\nhttp://docs.ansible.com/ansible/playbooks_error_handling.html\n\n\nstart-at-task\nansible-playbook playbook.yml --start-at-task=\"install packages\"\n\n\nStep\nPlaybooks can also be executed interactively with --step:\n\n\nansible-playbook playbook.yml --step\nThis will cause ansible to stop on each t\n\n\nhttp://docs.ansible.com/ansible/playbooks_startnstep.html", 
            "title": "Troubleshooting Techniques"
        }
    ]
}